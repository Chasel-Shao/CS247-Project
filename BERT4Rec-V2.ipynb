{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50805d26-b5ff-45e8-900e-c974eef809fe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Loaded 6040 users' movie interaction sequences\n",
      "Train users: 4832, Test users: 1208\n",
      "Epoch 1, Train Loss: 1.0553\n",
      "Epoch 2, Train Loss: 0.8319\n",
      "Epoch 3, Train Loss: 0.7952\n",
      "Epoch 4, Train Loss: 0.7655\n",
      "Epoch 5, Train Loss: 0.7251\n",
      "Epoch 6, Train Loss: 0.6756\n",
      "Epoch 7, Train Loss: 0.6245\n",
      "Epoch 8, Train Loss: 0.5653\n",
      "Epoch 9, Train Loss: 0.4989\n",
      "Epoch 10, Train Loss: 0.4092\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import BertModel, BertConfig\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# ✅ Load MovieLens-1M Data\n",
    "def load_data():\n",
    "    # Load ratings\n",
    "    ratings = pd.read_csv(\"ml-1m/ratings.dat\", sep=\"::\", engine=\"python\",\n",
    "                          names=[\"userId\", \"movieId\", \"rating\", \"timestamp\"],\n",
    "                          encoding=\"utf-8\")\n",
    "    ratings = ratings.sort_values(by=[\"userId\", \"timestamp\"])\n",
    "\n",
    "    # Load movies\n",
    "    movies = pd.read_csv(\"ml-1m/movies.dat\", sep=\"::\", engine=\"python\",\n",
    "                         names=[\"movieId\", \"title\", \"genres\"],\n",
    "                         encoding=\"latin-1\")  # ML-1M uses latin-1 encoding\n",
    "\n",
    "    # ✅ Define available genres\n",
    "    genre_list = [\n",
    "        \"Action\", \"Adventure\", \"Animation\", \"Children's\", \"Comedy\", \"Crime\",\n",
    "        \"Documentary\", \"Drama\", \"Fantasy\", \"Film-Noir\", \"Horror\", \"Musical\",\n",
    "        \"Mystery\", \"Romance\", \"Sci-Fi\", \"Thriller\", \"War\", \"Western\"\n",
    "    ]\n",
    "    genre_dict = {genre: idx + 1 for idx, genre in enumerate(genre_list)}\n",
    "\n",
    "    # ✅ Convert genres into numerical vectors\n",
    "    movies[\"genre_vector\"] = movies[\"genres\"].apply(\n",
    "        lambda x: [genre_dict[g] for g in x.split(\"|\") if g in genre_dict]\n",
    "    )\n",
    "    movie_dict = movies.set_index(\"movieId\")[\"genre_vector\"].to_dict()\n",
    "\n",
    "    # ✅ Create user-movie interaction dictionary\n",
    "    user_movie_dict = ratings.groupby(\"userId\")[\"movieId\"].apply(list).to_dict()\n",
    "\n",
    "    return user_movie_dict, movie_dict, genre_dict\n",
    "\n",
    "user_movie_dict, movie_dict, genre_dict = load_data()\n",
    "print(f\"Loaded {len(user_movie_dict)} users' movie interaction sequences\")\n",
    "\n",
    "# ✅ （Negative Sampling）\n",
    "def negative_sampling(movie_list, vocab_size, num_neg=5):\n",
    "    neg_samples = []\n",
    "    for movie in movie_list:\n",
    "        neg = []\n",
    "        while len(neg) < num_neg:\n",
    "            sampled = np.random.randint(1, vocab_size)\n",
    "            if sampled not in movie_list:\n",
    "                neg.append(sampled)\n",
    "        neg_samples.append(neg)\n",
    "    return neg_samples\n",
    "\n",
    "def split_train_test_strict(user_movie_dict, test_user_ratio=0.2, min_interactions=5):\n",
    "    users = list(user_movie_dict.keys())\n",
    "    np.random.shuffle(users)\n",
    "    split_idx = int(len(users) * (1 - test_user_ratio))\n",
    "    train_users = users[:split_idx]\n",
    "    test_users = users[split_idx:]\n",
    "\n",
    "    train_dict = {user: user_movie_dict[user] for user in train_users if len(user_movie_dict[user]) >= min_interactions}\n",
    "    test_dict = {user: user_movie_dict[user] for user in test_users if len(user_movie_dict[user]) >= min_interactions}\n",
    "\n",
    "    return train_dict, test_dict\n",
    "\n",
    "train_dict, test_dict = split_train_test_strict(user_movie_dict)\n",
    "print(f\"Train users: {len(train_dict)}, Test users: {len(test_dict)}\")\n",
    "\n",
    "class MovieDataset(Dataset):\n",
    "    def __init__(self, user_movie_dict, movie_dict, vocab_size, max_len=30, max_genres=5, num_neg=5):\n",
    "        self.users = list(user_movie_dict.keys())\n",
    "        self.sequences = [user_movie_dict[user] for user in self.users]\n",
    "        self.movie_dict = movie_dict\n",
    "        self.vocab_size = vocab_size\n",
    "        self.max_len = max_len\n",
    "        self.max_genres = max_genres\n",
    "        self.num_neg = num_neg  # Fixed number of negative samples per target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.users)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence = self.sequences[idx]\n",
    "        input_ids = sequence[:self.max_len] + [0] * (self.max_len - len(sequence))\n",
    "        target_ids = input_ids[1:] + [0]\n",
    "\n",
    "        # ✅ Fixed-shape negative sampling (max_len, num_neg)\n",
    "        neg_samples = []\n",
    "        for _ in range(self.max_len):\n",
    "            neg = []\n",
    "            while len(neg) < self.num_neg:\n",
    "                sampled = np.random.randint(1, self.vocab_size)\n",
    "                if sampled not in sequence:\n",
    "                    neg.append(sampled)\n",
    "            neg_samples.append(neg)\n",
    "\n",
    "        neg_samples = torch.tensor(neg_samples, dtype=torch.long)  # Shape: (max_len, num_neg)\n",
    "\n",
    "        attention_mask = [1 if id != 0 else 0 for id in input_ids]\n",
    "\n",
    "        genre_vectors = [self.movie_dict.get(movie, [0]) for movie in input_ids]\n",
    "        padded_genre_vectors = [g[:self.max_genres] + [0] * (self.max_genres - len(g)) for g in genre_vectors]\n",
    "\n",
    "        return (\n",
    "            torch.tensor(input_ids, dtype=torch.long),\n",
    "            torch.tensor(target_ids, dtype=torch.long),\n",
    "            neg_samples,  # ✅ Fixed-size tensor (max_len, num_neg)\n",
    "            torch.tensor(attention_mask, dtype=torch.long),\n",
    "            torch.tensor(padded_genre_vectors, dtype=torch.long)\n",
    "        )\n",
    "\n",
    "\n",
    "vocab_size = max(max(seq) for seq in user_movie_dict.values()) + 1\n",
    "train_dataset = MovieDataset(train_dict, movie_dict, vocab_size)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, num_workers=0, shuffle=True)\n",
    "test_dataset = MovieDataset(test_dict, movie_dict, vocab_size)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, num_workers=0, shuffle=False)\n",
    "\n",
    "# ✅ BERT4Rec\n",
    "class BERT4Rec(nn.Module):\n",
    "    def __init__(self, vocab_size, genre_size, hidden_size=256, num_layers=4, num_heads=4, max_len=30, dropout_rate=0.2):\n",
    "        super(BERT4Rec, self).__init__()\n",
    "        config = BertConfig(\n",
    "            vocab_size=vocab_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_attention_heads=num_heads,\n",
    "            num_hidden_layers=num_layers,\n",
    "            max_position_embeddings=max_len,\n",
    "        )\n",
    "        self.bert = BertModel(config)\n",
    "        self.output_layer = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "        self.genre_embedding = nn.Embedding(genre_size, hidden_size)\n",
    "        self.genre_fc = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "        self.layernorm = nn.LayerNorm(hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, genre_ids):\n",
    "        seq_len = input_ids.shape[1]\n",
    "        causal_mask = torch.triu(torch.ones(seq_len, seq_len, dtype=torch.bool, device=input_ids.device), diagonal=1)\n",
    "\n",
    "        output = self.bert(input_ids=input_ids, attention_mask=attention_mask, encoder_attention_mask=~causal_mask).last_hidden_state\n",
    "        genre_emb = self.genre_embedding(genre_ids).mean(dim=2)\n",
    "        genre_emb = self.genre_fc(genre_emb)\n",
    "\n",
    "        output = self.layernorm(output + genre_emb)\n",
    "        output = self.dropout(output)\n",
    "\n",
    "        return self.output_layer(output)\n",
    "\n",
    "model = BERT4Rec(vocab_size, len(genre_dict) + 1, dropout_rate=0.3).to(device)\n",
    "\n",
    "# ✅ Early Stopping Implementation\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=3, delta=0):\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.best_loss = float(\"inf\")\n",
    "        self.counter = 0\n",
    "\n",
    "    def step(self, val_loss):\n",
    "        if val_loss < self.best_loss - self.delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "# ✅ Training with Early Stopping\n",
    "def train_model(model, dataloader, epochs=10, lr=0.0001):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n",
    "    early_stopping = EarlyStopping(patience=3)\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for inputs, targets, neg_samples, attention_mask, genres in dataloader:\n",
    "            inputs, targets, neg_samples, attention_mask, genres = (\n",
    "                inputs.to(device), targets.to(device), neg_samples.to(device),\n",
    "                attention_mask.to(device), genres.to(device)\n",
    "            )\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs, attention_mask, genres)\n",
    "\n",
    "            pos_logits = outputs.gather(2, targets.unsqueeze(-1)).squeeze(-1)\n",
    "            neg_logits = outputs.gather(2, neg_samples).squeeze(-1)\n",
    "\n",
    "            pos_loss = criterion(pos_logits, torch.ones_like(pos_logits))\n",
    "            neg_loss = criterion(neg_logits, torch.zeros_like(neg_logits))\n",
    "            loss = pos_loss + neg_loss.mean()\n",
    "\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        scheduler.step(total_loss)\n",
    "        print(f\"Epoch {epoch + 1}, Train Loss: {total_loss / len(dataloader):.4f}\")\n",
    "\n",
    "        if early_stopping.step(total_loss):\n",
    "            break\n",
    "        \n",
    "\n",
    "train_model(model, train_dataloader, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b29b7845-41bd-4b79-8e78-840be41243ae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 5.6626, Recall@10: 0.2270, NDCG@10: 0.1253\n"
     ]
    }
   ],
   "source": [
    "def recall_at_k(top_k_predictions, targets, k):\n",
    "    \"\"\"\n",
    "    Compute Recall@K: \n",
    "    - Measures how many of the relevant items (targets) are in the top K recommendations.\n",
    "\n",
    "    Args:\n",
    "        top_k_predictions: (batch_size, seq_len, k) - Top K predicted items.\n",
    "        targets: (batch_size, seq_len) - True target items.\n",
    "        k: The number of top items considered.\n",
    "    \n",
    "    Returns:\n",
    "        Average Recall@K across all samples.\n",
    "    \"\"\"\n",
    "    hits = (top_k_predictions == targets.unsqueeze(-1)).float()  # Check if target is in top K\n",
    "    recall = hits.sum(dim=-1).mean().item()  # Compute recall\n",
    "    return recall\n",
    "\n",
    "def ndcg_at_k(top_k_predictions, targets, k):\n",
    "    \"\"\"\n",
    "    Compute NDCG@K:\n",
    "    - Measures ranking quality of recommendations by discounting correct predictions at later ranks.\n",
    "\n",
    "    Args:\n",
    "        top_k_predictions: (batch_size, seq_len, k) - Top K predicted items.\n",
    "        targets: (batch_size, seq_len) - True target items.\n",
    "        k: The number of top items considered.\n",
    "\n",
    "    Returns:\n",
    "        Average NDCG@K across all samples.\n",
    "    \"\"\"\n",
    "    hits = (top_k_predictions == targets.unsqueeze(-1)).float()\n",
    "    log_positions = 1 / torch.log2(torch.arange(2, k + 2, device=targets.device).float())  # Discount factor\n",
    "    dcg = (hits * log_positions).sum(dim=-1).mean().item()\n",
    "    return dcg\n",
    "\n",
    "# Define Evaluation Function\n",
    "def evaluate_model(model, dataloader, k=10):\n",
    "    model.eval()\n",
    "    total_loss, total_recall, total_ndcg = 0, 0, 0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets, _, attention_mask, genres in dataloader:\n",
    "            inputs, targets, attention_mask, genres = inputs.to(device), targets.to(device), attention_mask.to(device), genres.to(device)\n",
    "\n",
    "            outputs = model(inputs, attention_mask, genres)\n",
    "            loss = criterion(outputs.view(-1, outputs.shape[-1]), targets.view(-1))\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            _, top_k_predictions = torch.topk(outputs, k, dim=-1)\n",
    "            recall = recall_at_k(top_k_predictions, targets, k)\n",
    "            ndcg = ndcg_at_k(top_k_predictions, targets, k)\n",
    "\n",
    "            total_recall += recall\n",
    "            total_ndcg += ndcg\n",
    "\n",
    "    print(f\"Test Loss: {total_loss / len(dataloader):.4f}, Recall@{k}: {total_recall / len(dataloader):.4f}, NDCG@{k}: {total_ndcg / len(dataloader):.4f}\")\n",
    "\n",
    "# Evaluate Model\n",
    "evaluate_model(model, test_dataloader, k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38252279-cb13-43fb-8ee3-dc50406d0d9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
