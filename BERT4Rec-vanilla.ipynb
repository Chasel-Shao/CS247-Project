{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8894cf9b-675f-4889-b850-4fc3bedd3373",
   "metadata": {
    "tags": []
   },
   "source": [
    "# BERT4Rec-based recommendation system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3df273d-8360-4a05-9e81-ac0f39e882dc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import BertModel, BertConfig\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Ensure compatibility with Jupyter Notebook\n",
    "%matplotlib inline\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28d2aeec-b5e2-4a87-9234-e8d8ba3a3e40",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 6040 users' movie interaction sequences\n",
      "User 1: [3186, 1270, 1721, 1022, 2340, 1836, 3408, 2804, 1207, 1193, 720, 260, 919, 608, 2692, 1961, 2028, 3105, 938, 1035, 1962, 2018, 150, 1028, 1097, 914, 1287, 2797, 2762, 1246, 661, 2918, 531, 3114, 2791, 2321, 1029, 1197, 594, 2398, 1545, 527, 595, 2687, 745, 588, 1, 2355, 2294, 783, 1566, 1907, 48]\n",
      "User 2: [1198, 1210, 1217, 2717, 1293, 2943, 1225, 1193, 318, 3030, 2858, 1213, 1945, 1207, 593, 3095, 3468, 1873, 515, 1090, 2501, 3035, 110, 2067, 3147, 1247, 3105, 1357, 1196, 1957, 1953, 920, 1834, 1084, 1962, 3471, 3654, 3735, 1259, 1954, 1784, 2728, 1968, 1103, 902, 3451, 3578, 2852, 3334, 3068, 265, 2312, 590, 1253, 3071, 1244, 3699, 1955, 1245, 2236, 3678, 982, 2194, 2268, 1442, 3255, 647, 235, 1096, 1124, 498, 1246, 3893, 1537, 1188, 2396, 2359, 2321, 356, 3108, 1265, 3809, 589, 2028, 2571, 457, 2916, 1610, 480, 163, 380, 3418, 3256, 1408, 21, 349, 1527, 2353, 2006, 2278, 1370, 648, 2427, 1792, 1372, 1552, 2490, 1385, 780, 2881, 3107, 368, 1801, 165, 459, 442, 1597, 2628, 1690, 3257, 736, 2002, 2126, 292, 95, 1687, 434, 1544, 1917]\n",
      "User 3: [593, 2858, 3534, 1968, 1431, 1961, 1266, 1378, 1379, 3671, 590, 260, 1196, 2871, 1197, 1198, 3168, 1210, 1291, 2167, 1580, 1261, 480, 1615, 653, 733, 2006, 2470, 2115, 1049, 552, 2617, 648, 2735, 1136, 3114, 3421, 1394, 2997, 1304, 3619, 1270, 1079, 1259, 1265, 1641, 2355, 3552, 104, 3868, 2081]\n"
     ]
    }
   ],
   "source": [
    "# Function to load the MovieLens dataset\n",
    "def load_data(filepath=\"ml-1m/ratings.dat\"):\n",
    "    df = pd.read_csv(filepath, sep=\"::\", engine=\"python\", \n",
    "                     names=[\"userId\", \"movieId\", \"rating\", \"timestamp\"])\n",
    "    df = df.sort_values(by=[\"userId\", \"timestamp\"])  # Sort by user and timestamp\n",
    "    user_movie_dict = df.groupby(\"userId\")[\"movieId\"].apply(list).to_dict()\n",
    "    return user_movie_dict\n",
    "\n",
    "# Load dataset\n",
    "user_movie_dict = load_data()\n",
    "print(f\"Loaded {len(user_movie_dict)} users' movie interaction sequences\")\n",
    "\n",
    "# Print a sample of user-movie interactions for debugging\n",
    "for user, movies in list(user_movie_dict.items())[:3]:\n",
    "    print(f\"User {user}: {movies}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4ec826b-5b5d-4be2-a707-5566240946e0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train users: 6040, Test users: 6040\n"
     ]
    }
   ],
   "source": [
    "# Function to split user interactions into train and test sets\n",
    "def split_train_test(user_movie_dict, test_ratio=0.2, min_interactions=5):\n",
    "    train_dict, test_dict = {}, {}\n",
    "\n",
    "    for user, movies in user_movie_dict.items():\n",
    "        if len(movies) >= min_interactions:  # Only split users with enough data\n",
    "            split_idx = int(len(movies) * (1 - test_ratio))\n",
    "            train_dict[user] = movies[:split_idx]\n",
    "            test_dict[user] = movies[split_idx:]\n",
    "        else:\n",
    "            train_dict[user] = movies  # Assign all to train if only a few interactions\n",
    "    \n",
    "    return train_dict, test_dict\n",
    "\n",
    "# Apply train-test split with filtering\n",
    "train_dict, test_dict = split_train_test(user_movie_dict, test_ratio=0.2, min_interactions=5)\n",
    "\n",
    "# Print updated user counts\n",
    "print(f\"Train users: {len(train_dict)}, Test users: {len(test_dict)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54c3ea7d-c557-48e2-bf1e-0eb321619cc2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dataset: 6040, Test Dataset: 6040\n"
     ]
    }
   ],
   "source": [
    "# Define Dataset for Training\n",
    "class MovieDataset(Dataset):\n",
    "    def __init__(self, user_movie_dict, max_len=50):\n",
    "        self.users = list(user_movie_dict.keys())\n",
    "        self.sequences = [user_movie_dict[user] for user in self.users]\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.users)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence = self.sequences[idx]\n",
    "        input_ids = sequence[:self.max_len] + [0] * (self.max_len - len(sequence))  # Padding\n",
    "        target_ids = input_ids[1:] + [0]  # Next-movie prediction\n",
    "        attention_mask = [1 if id != 0 else 0 for id in input_ids]  # Attention mask\n",
    "        return torch.tensor(input_ids), torch.tensor(target_ids), torch.tensor(attention_mask)\n",
    "\n",
    "# Create Train Dataset & DataLoader\n",
    "train_dataset = MovieDataset(train_dict, max_len=50)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Create Test Dataset & DataLoader\n",
    "test_dataset = MovieDataset(test_dict, max_len=50)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "print(f\"Train Dataset: {len(train_dataset)}, Test Dataset: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3917c9d-f731-4bf0-9a85-98c36f1ef916",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized BERT4Rec model with vocab size 3953\n"
     ]
    }
   ],
   "source": [
    "# Transformer-based BERT4Rec Model\n",
    "class BERT4Rec(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size=256, num_layers=4, num_heads=4, max_len=50):\n",
    "        super(BERT4Rec, self).__init__()\n",
    "        config = BertConfig(\n",
    "            vocab_size=vocab_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_attention_heads=num_heads,\n",
    "            num_hidden_layers=num_layers,\n",
    "            max_position_embeddings=max_len,\n",
    "        )\n",
    "        self.bert = BertModel(config)\n",
    "        self.output_layer = nn.Linear(hidden_size, vocab_size)  # Output layer to predict next movie ID\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        output = self.bert(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state\n",
    "        return self.output_layer(output)\n",
    "\n",
    "# Initialize Model\n",
    "vocab_size = max(max(seq) for seq in user_movie_dict.values()) + 1  # Get max movie ID as vocab size\n",
    "model = BERT4Rec(vocab_size).to(device)\n",
    "\n",
    "print(f\"Initialized BERT4Rec model with vocab size {vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d942151-25fd-4879-a9a6-89dbd2d9c76f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 6.129596157679482\n",
      "Epoch 2, Train Loss: 5.767031051494457\n",
      "Epoch 3, Train Loss: 4.2274885593898714\n"
     ]
    }
   ],
   "source": [
    "# Define Training Function\n",
    "def train_model(model, dataloader, epochs=3, lr=0.001):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for inputs, targets, attention_mask in dataloader:\n",
    "            inputs, targets, attention_mask = inputs.to(device), targets.to(device), attention_mask.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs, attention_mask)\n",
    "            loss = criterion(outputs.view(-1, outputs.shape[-1]), targets.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        print(f\"Epoch {epoch + 1}, Train Loss: {total_loss / len(dataloader)}\")\n",
    "\n",
    "# Train on Train DataLoader\n",
    "train_model(model, train_dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f11f335-c400-48d4-bb06-421207cfd204",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define Evaluation Function\n",
    "def evaluate_model(model, dataloader, k=10):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_recall = 0\n",
    "    total_ndcg = 0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets, attention_mask in dataloader:\n",
    "            inputs, targets, attention_mask = inputs.to(device), targets.to(device), attention_mask.to(device)\n",
    "            \n",
    "            outputs = model(inputs, attention_mask)  # (batch_size, seq_len, vocab_size)\n",
    "            loss = criterion(outputs.view(-1, outputs.shape[-1]), targets.view(-1))\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Compute Recall@K & NDCG@K\n",
    "            _, top_k_predictions = torch.topk(outputs, k, dim=-1)  # Get top K movie predictions\n",
    "            recall = recall_at_k(top_k_predictions, targets, k)\n",
    "            ndcg = ndcg_at_k(top_k_predictions, targets, k)\n",
    "\n",
    "            total_recall += recall\n",
    "            total_ndcg += ndcg\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    avg_recall = total_recall / len(dataloader)\n",
    "    avg_ndcg = total_ndcg / len(dataloader)\n",
    "\n",
    "    print(f\"Test Loss: {avg_loss:.4f}, Recall@{k}: {avg_recall:.4f}, NDCG@{k}: {avg_ndcg:.4f}\")\n",
    "    return avg_loss, avg_recall, avg_ndcg\n",
    "\n",
    "# Compute Recall@K\n",
    "def recall_at_k(top_k_predictions, targets, k):\n",
    "    hits = (top_k_predictions == targets.unsqueeze(-1)).float()  # Check if target is in top K\n",
    "    recall = hits.sum(dim=-1).mean().item()  # Compute recall\n",
    "    return recall\n",
    "\n",
    "# Compute NDCG@K\n",
    "def ndcg_at_k(top_k_predictions, targets, k):\n",
    "    hits = (top_k_predictions == targets.unsqueeze(-1)).float()\n",
    "    log_positions = 1 / torch.log2(torch.arange(2, k + 2, device=targets.device).float())  # Discount factor\n",
    "    dcg = (hits * log_positions).sum(dim=-1).mean().item()\n",
    "    return dcg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c83a1d4-b53a-4bd0-84b6-60d6728b88bc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 2.4125, Recall@10: 0.7370, NDCG@10: 0.6553\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on Test DataLoader\n",
    "test_loss, recall_k, ndcg_k = evaluate_model(model, test_dataloader, k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f049d05-3e8e-45dd-b9e8-7ef92e13360e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
