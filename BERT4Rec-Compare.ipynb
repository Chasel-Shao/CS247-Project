{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "749b7da8-317a-428e-948b-bb63dc19eaed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import BertModel, BertConfig\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# ✅ Load MovieLens-1M Data\n",
    "def load_data():\n",
    "    # Load ratings\n",
    "    ratings = pd.read_csv(\"ml-1m/ratings.dat\", sep=\"::\", engine=\"python\",\n",
    "                          names=[\"userId\", \"movieId\", \"rating\", \"timestamp\"],\n",
    "                          encoding=\"utf-8\")\n",
    "    ratings = ratings.sort_values(by=[\"userId\", \"timestamp\"])\n",
    "\n",
    "    # Load movies\n",
    "    movies = pd.read_csv(\"ml-1m/movies.dat\", sep=\"::\", engine=\"python\",\n",
    "                         names=[\"movieId\", \"title\", \"genres\"],\n",
    "                         encoding=\"latin-1\")  # ML-1M uses latin-1 encoding\n",
    "\n",
    "    # ✅ Define available genres\n",
    "    genre_list = [\n",
    "        \"Action\", \"Adventure\", \"Animation\", \"Children's\", \"Comedy\", \"Crime\",\n",
    "        \"Documentary\", \"Drama\", \"Fantasy\", \"Film-Noir\", \"Horror\", \"Musical\",\n",
    "        \"Mystery\", \"Romance\", \"Sci-Fi\", \"Thriller\", \"War\", \"Western\"\n",
    "    ]\n",
    "    genre_dict = {genre: idx + 1 for idx, genre in enumerate(genre_list)}\n",
    "\n",
    "    # ✅ Convert genres into numerical vectors\n",
    "    movies[\"genre_vector\"] = movies[\"genres\"].apply(\n",
    "        lambda x: [genre_dict[g] for g in x.split(\"|\") if g in genre_dict]\n",
    "    )\n",
    "    movie_dict = movies.set_index(\"movieId\")[\"genre_vector\"].to_dict()\n",
    "\n",
    "    # ✅ Create user-movie interaction dictionary\n",
    "    user_movie_dict = ratings.groupby(\"userId\")[\"movieId\"].apply(list).to_dict()\n",
    "\n",
    "    return user_movie_dict, movie_dict, genre_dict\n",
    "\n",
    "# ✅ （Negative Sampling）\n",
    "def negative_sampling(movie_list, vocab_size, num_neg=5):\n",
    "    neg_samples = []\n",
    "    for movie in movie_list:\n",
    "        neg = []\n",
    "        while len(neg) < num_neg:\n",
    "            sampled = np.random.randint(1, vocab_size)\n",
    "            if sampled not in movie_list:\n",
    "                neg.append(sampled)\n",
    "        neg_samples.append(neg)\n",
    "    return neg_samples\n",
    "\n",
    "def split_train_test_strict(user_movie_dict, test_user_ratio=0.2, min_interactions=5):\n",
    "    users = list(user_movie_dict.keys())\n",
    "    np.random.shuffle(users)\n",
    "    split_idx = int(len(users) * (1 - test_user_ratio))\n",
    "    train_users = users[:split_idx]\n",
    "    test_users = users[split_idx:]\n",
    "\n",
    "    train_dict = {user: user_movie_dict[user] for user in train_users if len(user_movie_dict[user]) >= min_interactions}\n",
    "    test_dict = {user: user_movie_dict[user] for user in test_users if len(user_movie_dict[user]) >= min_interactions}\n",
    "\n",
    "    return train_dict, test_dict\n",
    "\n",
    "# ✅ Early Stopping Implementation\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=3, delta=0):\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.best_loss = float(\"inf\")\n",
    "        self.counter = 0\n",
    "\n",
    "    def step(self, val_loss):\n",
    "        if val_loss < self.best_loss - self.delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "def recall_at_k(top_k_predictions, targets, k):\n",
    "    \"\"\"\n",
    "    Compute Recall@K: \n",
    "    - Measures how many of the relevant items (targets) are in the top K recommendations.\n",
    "\n",
    "    Args:\n",
    "        top_k_predictions: (batch_size, seq_len, k) - Top K predicted items.\n",
    "        targets: (batch_size, seq_len) - True target items.\n",
    "        k: The number of top items considered.\n",
    "    \n",
    "    Returns:\n",
    "        Average Recall@K across all samples.\n",
    "    \"\"\"\n",
    "    hits = (top_k_predictions == targets.unsqueeze(-1)).float()  # Check if target is in top K\n",
    "    recall = hits.sum(dim=-1).mean().item()  # Compute recall\n",
    "    return recall\n",
    "\n",
    "def ndcg_at_k(top_k_predictions, targets, k):\n",
    "    \"\"\"\n",
    "    Compute NDCG@K:\n",
    "    - Measures ranking quality of recommendations by discounting correct predictions at later ranks.\n",
    "\n",
    "    Args:\n",
    "        top_k_predictions: (batch_size, seq_len, k) - Top K predicted items.\n",
    "        targets: (batch_size, seq_len) - True target items.\n",
    "        k: The number of top items considered.\n",
    "\n",
    "    Returns:\n",
    "        Average NDCG@K across all samples.\n",
    "    \"\"\"\n",
    "    hits = (top_k_predictions == targets.unsqueeze(-1)).float()\n",
    "    log_positions = 1 / torch.log2(torch.arange(2, k + 2, device=targets.device).float())  # Discount factor\n",
    "    dcg = (hits * log_positions).sum(dim=-1).mean().item()\n",
    "    return dcg\n",
    "\n",
    "epochs = 200\n",
    "losses_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "175c5e6d-531e-4ffb-a197-f81c781c9152",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 6040 users' movie interaction sequences\n",
      "Train users: 4832, Test users: 1208\n",
      "Epoch 1, Train Loss: 1.0829\n",
      "Epoch 2, Train Loss: 0.8475\n",
      "Epoch 3, Train Loss: 0.8103\n",
      "Epoch 4, Train Loss: 0.7828\n",
      "Epoch 5, Train Loss: 0.7473\n",
      "Epoch 6, Train Loss: 0.6993\n",
      "Epoch 7, Train Loss: 0.6543\n",
      "Epoch 8, Train Loss: 0.6029\n",
      "Epoch 9, Train Loss: 0.5485\n",
      "Epoch 10, Train Loss: 0.4827\n",
      "Epoch 11, Train Loss: 0.4010\n",
      "Epoch 12, Train Loss: 0.3439\n",
      "Epoch 13, Train Loss: 0.3007\n",
      "Epoch 14, Train Loss: 0.2667\n",
      "Epoch 15, Train Loss: 0.2386\n",
      "Epoch 16, Train Loss: 0.2134\n",
      "Epoch 17, Train Loss: 0.1932\n",
      "Epoch 18, Train Loss: 0.1756\n",
      "Epoch 19, Train Loss: 0.1609\n",
      "Epoch 20, Train Loss: 0.1475\n",
      "Epoch 21, Train Loss: 0.1365\n",
      "Epoch 22, Train Loss: 0.1262\n",
      "Epoch 23, Train Loss: 0.1167\n",
      "Epoch 24, Train Loss: 0.1083\n",
      "Epoch 25, Train Loss: 0.1014\n",
      "Epoch 26, Train Loss: 0.0954\n",
      "Epoch 27, Train Loss: 0.0891\n",
      "Epoch 28, Train Loss: 0.0839\n",
      "Epoch 29, Train Loss: 0.0794\n",
      "Epoch 30, Train Loss: 0.0754\n",
      "Epoch 31, Train Loss: 0.0715\n",
      "Epoch 32, Train Loss: 0.0682\n",
      "Epoch 33, Train Loss: 0.0651\n",
      "Epoch 34, Train Loss: 0.0623\n",
      "Epoch 35, Train Loss: 0.0594\n",
      "Epoch 36, Train Loss: 0.0571\n",
      "Epoch 37, Train Loss: 0.0551\n",
      "Epoch 38, Train Loss: 0.0528\n",
      "Epoch 39, Train Loss: 0.0515\n",
      "Epoch 40, Train Loss: 0.0499\n",
      "Epoch 41, Train Loss: 0.0482\n",
      "Epoch 42, Train Loss: 0.0472\n",
      "Epoch 43, Train Loss: 0.0458\n",
      "Epoch 44, Train Loss: 0.0445\n",
      "Epoch 45, Train Loss: 0.0435\n",
      "Epoch 46, Train Loss: 0.0422\n",
      "Epoch 47, Train Loss: 0.0416\n",
      "Epoch 48, Train Loss: 0.0408\n",
      "Epoch 49, Train Loss: 0.0399\n",
      "Epoch 50, Train Loss: 0.0394\n",
      "Epoch 51, Train Loss: 0.0387\n",
      "Epoch 52, Train Loss: 0.0382\n",
      "Epoch 53, Train Loss: 0.0374\n",
      "Epoch 54, Train Loss: 0.0370\n",
      "Epoch 55, Train Loss: 0.0364\n",
      "Epoch 56, Train Loss: 0.0362\n",
      "Epoch 57, Train Loss: 0.0358\n",
      "Epoch 58, Train Loss: 0.0350\n",
      "Epoch 59, Train Loss: 0.0348\n",
      "Epoch 60, Train Loss: 0.0344\n",
      "Epoch 61, Train Loss: 0.0345\n",
      "Epoch 62, Train Loss: 0.0338\n",
      "Epoch 63, Train Loss: 0.0335\n",
      "Epoch 64, Train Loss: 0.0333\n",
      "Epoch 65, Train Loss: 0.0331\n",
      "Epoch 66, Train Loss: 0.0329\n",
      "Epoch 67, Train Loss: 0.0325\n",
      "Epoch 68, Train Loss: 0.0323\n",
      "Epoch 69, Train Loss: 0.0319\n",
      "Epoch 70, Train Loss: 0.0321\n",
      "Epoch 71, Train Loss: 0.0319\n",
      "Epoch 72, Train Loss: 0.0316\n",
      "Epoch 73, Train Loss: 0.0317\n",
      "Epoch 74, Train Loss: 0.0316\n",
      "Epoch 75, Train Loss: 0.0314\n",
      "Epoch 76, Train Loss: 0.0312\n",
      "Epoch 77, Train Loss: 0.0310\n",
      "Epoch 78, Train Loss: 0.0309\n",
      "Epoch 79, Train Loss: 0.0307\n",
      "Epoch 80, Train Loss: 0.0306\n",
      "Epoch 81, Train Loss: 0.0305\n",
      "Epoch 82, Train Loss: 0.0307\n",
      "Epoch 83, Train Loss: 0.0304\n",
      "Epoch 84, Train Loss: 0.0303\n",
      "Epoch 85, Train Loss: 0.0302\n",
      "Epoch 86, Train Loss: 0.0301\n",
      "Epoch 87, Train Loss: 0.0300\n",
      "Epoch 88, Train Loss: 0.0300\n",
      "Epoch 89, Train Loss: 0.0299\n",
      "Epoch 90, Train Loss: 0.0299\n",
      "Epoch 91, Train Loss: 0.0298\n",
      "Epoch 92, Train Loss: 0.0299\n",
      "Epoch 93, Train Loss: 0.0299\n",
      "Epoch 94, Train Loss: 0.0295\n",
      "Epoch 95, Train Loss: 0.0296\n",
      "Epoch 96, Train Loss: 0.0297\n",
      "Epoch 97, Train Loss: 0.0293\n",
      "Epoch 98, Train Loss: 0.0293\n",
      "Epoch 99, Train Loss: 0.0293\n",
      "Epoch 100, Train Loss: 0.0291\n",
      "Epoch 101, Train Loss: 0.0290\n",
      "Epoch 102, Train Loss: 0.0290\n",
      "Epoch 103, Train Loss: 0.0290\n",
      "Epoch 104, Train Loss: 0.0287\n",
      "Epoch 105, Train Loss: 0.0289\n",
      "Epoch 106, Train Loss: 0.0287\n",
      "Epoch 107, Train Loss: 0.0288\n",
      "Epoch 108, Train Loss: 0.0286\n",
      "Epoch 109, Train Loss: 0.0286\n",
      "Epoch 110, Train Loss: 0.0288\n",
      "Epoch 111, Train Loss: 0.0285\n",
      "Epoch 112, Train Loss: 0.0286\n",
      "Epoch 113, Train Loss: 0.0286\n",
      "Epoch 114, Train Loss: 0.0283\n",
      "Epoch 115, Train Loss: 0.0282\n",
      "Epoch 116, Train Loss: 0.0285\n",
      "Epoch 117, Train Loss: 0.0283\n",
      "Epoch 118, Train Loss: 0.0282\n",
      "Early stopping triggered.\n",
      "Test Loss: 2.8055, Recall@10: 0.9941, NDCG@10: 0.6467\n"
     ]
    }
   ],
   "source": [
    "#### BERT4Rec Baseline\n",
    "\n",
    "class MovieDataset_Baseline(Dataset):\n",
    "    def __init__(self, user_movie_dict, movie_dict, vocab_size, max_len=30, max_genres=5, num_neg=5):\n",
    "        self.users = list(user_movie_dict.keys())\n",
    "        self.sequences = [user_movie_dict[user] for user in self.users]\n",
    "        self.movie_dict = movie_dict\n",
    "        self.vocab_size = vocab_size\n",
    "        self.max_len = max_len\n",
    "        self.max_genres = max_genres\n",
    "        self.num_neg = num_neg  # Fixed number of negative samples per target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.users)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence = self.sequences[idx]\n",
    "        input_ids = sequence[:self.max_len] + [0] * (self.max_len - len(sequence))\n",
    "        target_ids = input_ids[1:] + [0]\n",
    "\n",
    "        # ✅ Fixed-shape negative sampling (max_len, num_neg)\n",
    "        neg_samples = []\n",
    "        for _ in range(self.max_len):\n",
    "            neg = []\n",
    "            while len(neg) < self.num_neg:\n",
    "                sampled = np.random.randint(1, self.vocab_size)\n",
    "                if sampled not in sequence:\n",
    "                    neg.append(sampled)\n",
    "            neg_samples.append(neg)\n",
    "\n",
    "        neg_samples = torch.tensor(neg_samples, dtype=torch.long)  # Shape: (max_len, num_neg)\n",
    "\n",
    "        attention_mask = [1 if id != 0 else 0 for id in input_ids]\n",
    "\n",
    "        genre_vectors = [self.movie_dict.get(movie, [0]) for movie in input_ids]\n",
    "        padded_genre_vectors = [g[:self.max_genres] + [0] * (self.max_genres - len(g)) for g in genre_vectors]\n",
    "\n",
    "        return (\n",
    "            torch.tensor(input_ids, dtype=torch.long),\n",
    "            torch.tensor(target_ids, dtype=torch.long),\n",
    "            neg_samples,  # ✅ Fixed-size tensor (max_len, num_neg)\n",
    "            torch.tensor(attention_mask, dtype=torch.long),\n",
    "            torch.tensor(padded_genre_vectors, dtype=torch.long)\n",
    "        )\n",
    "\n",
    "# ✅ BERT4Rec\n",
    "class BERT4Rec(nn.Module):\n",
    "    def __init__(self, vocab_size, genre_size, hidden_size=256, num_layers=4, num_heads=4, max_len=30, dropout_rate=0.2):\n",
    "        super(BERT4Rec, self).__init__()\n",
    "        config = BertConfig(\n",
    "            vocab_size=vocab_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_attention_heads=num_heads,\n",
    "            num_hidden_layers=num_layers,\n",
    "            max_position_embeddings=max_len,\n",
    "        )\n",
    "        self.bert = BertModel(config)\n",
    "        self.output_layer = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "        self.genre_embedding = nn.Embedding(genre_size, hidden_size)\n",
    "        self.genre_fc = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "        self.layernorm = nn.LayerNorm(hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, genre_ids):\n",
    "        seq_len = input_ids.shape[1]\n",
    "        # causal_mask = torch.triu(torch.ones(seq_len, seq_len, dtype=torch.bool, device=input_ids.device), diagonal=1)\n",
    "\n",
    "        output = self.bert(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state\n",
    "        genre_emb = self.genre_embedding(genre_ids).mean(dim=2)\n",
    "        genre_emb = self.genre_fc(genre_emb)\n",
    "\n",
    "        output = self.layernorm(output + genre_emb)\n",
    "        output = self.dropout(output)\n",
    "\n",
    "        return self.output_layer(output)\n",
    "\n",
    "# ✅ Training with Early Stopping\n",
    "def train_model(model, dataloader, epochs=10, lr=0.0001):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n",
    "    early_stopping = EarlyStopping(patience=3)\n",
    "\n",
    "    model.train()\n",
    "    loss_history = []\n",
    "        \n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for inputs, targets, neg_samples, attention_mask, genres in dataloader:\n",
    "            inputs, targets, neg_samples, attention_mask, genres = (\n",
    "                inputs.to(device), targets.to(device), neg_samples.to(device),\n",
    "                attention_mask.to(device), genres.to(device)\n",
    "            )\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs, attention_mask, genres)\n",
    "\n",
    "            pos_logits = outputs.gather(2, targets.unsqueeze(-1)).squeeze(-1)\n",
    "            neg_logits = outputs.gather(2, neg_samples).squeeze(-1)\n",
    "\n",
    "            pos_loss = criterion(pos_logits, torch.ones_like(pos_logits))\n",
    "            neg_loss = criterion(neg_logits, torch.zeros_like(neg_logits))\n",
    "            loss = pos_loss + neg_loss.mean()\n",
    "\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            \n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        loss_history.append(avg_loss)\n",
    "        scheduler.step(avg_loss)\n",
    "        print(f\"Epoch {epoch + 1}, Train Loss: {avg_loss:.4f}\")\n",
    "\n",
    "        if early_stopping.step(total_loss):\n",
    "            break\n",
    "            \n",
    "    return loss_history\n",
    "\n",
    "# Define Evaluation Function\n",
    "def evaluate_model_baseline(model, dataloader, k=10):\n",
    "    model.eval()\n",
    "    total_loss, total_recall, total_ndcg = 0, 0, 0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets, _, attention_mask, genres in dataloader:\n",
    "            inputs, targets, attention_mask, genres = inputs.to(device), targets.to(device), attention_mask.to(device), genres.to(device)\n",
    "\n",
    "            outputs = model(inputs, attention_mask, genres)\n",
    "            loss = criterion(outputs.view(-1, outputs.shape[-1]), targets.view(-1))\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            _, top_k_predictions = torch.topk(outputs, k, dim=-1)\n",
    "            recall = recall_at_k(top_k_predictions, targets, k)\n",
    "            ndcg = ndcg_at_k(top_k_predictions, targets, k)\n",
    "\n",
    "            total_recall += recall\n",
    "            total_ndcg += ndcg\n",
    "\n",
    "    print(f\"Test Loss: {total_loss / len(dataloader):.4f}, Recall@{k}: {total_recall / len(dataloader):.4f}, NDCG@{k}: {total_ndcg / len(dataloader):.4f}\")\n",
    "\n",
    "user_movie_dict, movie_dict, genre_dict = load_data()\n",
    "print(f\"Loaded {len(user_movie_dict)} users' movie interaction sequences\")\n",
    "train_dict, test_dict = split_train_test_strict(user_movie_dict)\n",
    "print(f\"Train users: {len(train_dict)}, Test users: {len(test_dict)}\")\n",
    "\n",
    "vocab_size = max(max(seq) for seq in user_movie_dict.values()) + 1\n",
    "train_dataset = MovieDataset_Baseline(train_dict, movie_dict, vocab_size)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, num_workers=0, shuffle=True)\n",
    "test_dataset = MovieDataset_Baseline(test_dict, movie_dict, vocab_size)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, num_workers=0, shuffle=False)\n",
    "            \n",
    "model_baseline = BERT4Rec(vocab_size, len(genre_dict) + 1, dropout_rate=0.4).to(device)\n",
    "loss_history_baseline = train_model(model_baseline, train_dataloader, epochs=epochs, lr=0.0001)\n",
    "# Evaluate Model\n",
    "evaluate_model_baseline(model_baseline, test_dataloader, k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8d4c7435-1b71-41f2-b1df-95de1617bafe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 6040 users' movie interaction sequences.\n",
      "Train users: 4832, Test users: 1208\n",
      "Model initialized with vocab size 3953\n",
      "Epoch 1, Train Loss: 7.9873\n",
      "Epoch 2, Train Loss: 7.2153\n",
      "Epoch 3, Train Loss: 6.7783\n",
      "Epoch 4, Train Loss: 6.2292\n",
      "Epoch 5, Train Loss: 4.9588\n",
      "Epoch 6, Train Loss: 3.2603\n",
      "Epoch 7, Train Loss: 2.1169\n",
      "Epoch 8, Train Loss: 1.4054\n",
      "Epoch 9, Train Loss: 0.9544\n",
      "Epoch 10, Train Loss: 0.6689\n",
      "Epoch 11, Train Loss: 0.4833\n",
      "Epoch 12, Train Loss: 0.3644\n",
      "Epoch 13, Train Loss: 0.2831\n",
      "Epoch 14, Train Loss: 0.2281\n",
      "Epoch 15, Train Loss: 0.1855\n",
      "Epoch 16, Train Loss: 0.1543\n",
      "Epoch 17, Train Loss: 0.1303\n",
      "Epoch 18, Train Loss: 0.1120\n",
      "Epoch 19, Train Loss: 0.0979\n",
      "Epoch 20, Train Loss: 0.0851\n",
      "Epoch 21, Train Loss: 0.0750\n",
      "Epoch 22, Train Loss: 0.0673\n",
      "Epoch 23, Train Loss: 0.0611\n",
      "Epoch 24, Train Loss: 0.0551\n",
      "Epoch 25, Train Loss: 0.0504\n",
      "Epoch 26, Train Loss: 0.0466\n",
      "Epoch 27, Train Loss: 0.0434\n",
      "Epoch 28, Train Loss: 0.0405\n",
      "Epoch 29, Train Loss: 0.0382\n",
      "Epoch 30, Train Loss: 0.0359\n",
      "Epoch 31, Train Loss: 0.0337\n",
      "Epoch 32, Train Loss: 0.0324\n",
      "Epoch 33, Train Loss: 0.0309\n",
      "Epoch 34, Train Loss: 0.0293\n",
      "Epoch 35, Train Loss: 0.0284\n",
      "Epoch 36, Train Loss: 0.0274\n",
      "Epoch 37, Train Loss: 0.0264\n",
      "Epoch 38, Train Loss: 0.0258\n",
      "Epoch 39, Train Loss: 0.0250\n",
      "Epoch 40, Train Loss: 0.0245\n",
      "Epoch 41, Train Loss: 0.0237\n",
      "Epoch 42, Train Loss: 0.0234\n",
      "Epoch 43, Train Loss: 0.0228\n",
      "Epoch 44, Train Loss: 0.0222\n",
      "Epoch 45, Train Loss: 0.0217\n",
      "Epoch 46, Train Loss: 0.0217\n",
      "Epoch 47, Train Loss: 0.0211\n",
      "Epoch 48, Train Loss: 0.0207\n",
      "Epoch 49, Train Loss: 0.0205\n",
      "Epoch 50, Train Loss: 0.0203\n",
      "Epoch 51, Train Loss: 0.0197\n",
      "Epoch 52, Train Loss: 0.0197\n",
      "Epoch 53, Train Loss: 0.0195\n",
      "Epoch 54, Train Loss: 0.0193\n",
      "Epoch 55, Train Loss: 0.0190\n",
      "Epoch 56, Train Loss: 0.0187\n",
      "Epoch 57, Train Loss: 0.0187\n",
      "Epoch 58, Train Loss: 0.0181\n",
      "Epoch 59, Train Loss: 0.0182\n",
      "Epoch 60, Train Loss: 0.0178\n",
      "Epoch 61, Train Loss: 0.0177\n",
      "Epoch 62, Train Loss: 0.0175\n",
      "Epoch 63, Train Loss: 0.0175\n",
      "Epoch 64, Train Loss: 0.0172\n",
      "Epoch 65, Train Loss: 0.0170\n",
      "Epoch 66, Train Loss: 0.0169\n",
      "Epoch 67, Train Loss: 0.0166\n",
      "Epoch 68, Train Loss: 0.0164\n",
      "Epoch 69, Train Loss: 0.0162\n",
      "Epoch 70, Train Loss: 0.0162\n",
      "Epoch 71, Train Loss: 0.0160\n",
      "Epoch 72, Train Loss: 0.0160\n",
      "Epoch 73, Train Loss: 0.0158\n",
      "Epoch 74, Train Loss: 0.0156\n",
      "Epoch 75, Train Loss: 0.0154\n",
      "Epoch 76, Train Loss: 0.0154\n",
      "Epoch 77, Train Loss: 0.0156\n",
      "Epoch 78, Train Loss: 0.0151\n",
      "Epoch 79, Train Loss: 0.0151\n",
      "Epoch 80, Train Loss: 0.0151\n",
      "Epoch 81, Train Loss: 0.0149\n",
      "Epoch 82, Train Loss: 0.0147\n",
      "Epoch 83, Train Loss: 0.0146\n",
      "Epoch 84, Train Loss: 0.0146\n",
      "Epoch 85, Train Loss: 0.0144\n",
      "Epoch 86, Train Loss: 0.0144\n",
      "Epoch 87, Train Loss: 0.0142\n",
      "Epoch 88, Train Loss: 0.0141\n",
      "Epoch 89, Train Loss: 0.0143\n",
      "Epoch 90, Train Loss: 0.0143\n",
      "Epoch 91, Train Loss: 0.0138\n",
      "Epoch 92, Train Loss: 0.0140\n",
      "Epoch 93, Train Loss: 0.0141\n",
      "Epoch 94, Train Loss: 0.0138\n",
      "Epoch 95, Train Loss: 0.0134\n",
      "Epoch 96, Train Loss: 0.0137\n",
      "Epoch 97, Train Loss: 0.0135\n",
      "Epoch 98, Train Loss: 0.0137\n",
      "Early stopping triggered.\n",
      "Test Loss: 0.0325, Recall@10: 0.9376, NDCG@10: 0.9376\n"
     ]
    }
   ],
   "source": [
    "### BERT4Rect-UP\n",
    "\n",
    "# ✅ Load MovieLens-1M Data (Users, Ratings, Movies)\n",
    "def load_ratings(filepath=\"ml-1m/ratings.dat\"):\n",
    "    df = pd.read_csv(filepath, sep=\"::\", engine=\"python\",\n",
    "                     names=[\"userId\", \"movieId\", \"rating\", \"timestamp\"],\n",
    "                     encoding=\"utf-8\")\n",
    "    df = df.sort_values(by=[\"userId\", \"timestamp\"])\n",
    "    user_movie_dict = df.groupby(\"userId\")[\"movieId\"].apply(list).to_dict()\n",
    "    return user_movie_dict, df\n",
    "\n",
    "def load_users(filepath=\"ml-1m/users.dat\"):\n",
    "    user_df = pd.read_csv(filepath, sep=\"::\", engine=\"python\",\n",
    "                           names=[\"userId\", \"gender\", \"age\", \"occupation\", \"zip-code\"],\n",
    "                           encoding=\"utf-8\")\n",
    "\n",
    "    gender_map = {\"M\": 0, \"F\": 1}\n",
    "    user_df[\"gender\"] = user_df[\"gender\"].map(gender_map)\n",
    "\n",
    "    age_groups = {1: 0, 18: 1, 25: 2, 35: 3, 45: 4, 50: 5, 56: 6}\n",
    "    user_df[\"age\"] = user_df[\"age\"].map(age_groups)\n",
    "\n",
    "    users_dict = user_df.set_index(\"userId\")[[\"gender\", \"age\", \"occupation\"]].to_dict(\"index\")\n",
    "    return users_dict\n",
    "\n",
    "def load_movies(filepath=\"ml-1m/movies.dat\"):\n",
    "    movies = pd.read_csv(filepath, sep=\"::\", engine=\"python\",\n",
    "                         names=[\"movieId\", \"title\", \"genres\"], encoding=\"latin-1\")\n",
    "\n",
    "    genre_list = [\n",
    "        \"Action\", \"Adventure\", \"Animation\", \"Children's\", \"Comedy\", \"Crime\",\n",
    "        \"Documentary\", \"Drama\", \"Fantasy\", \"Film-Noir\", \"Horror\", \"Musical\",\n",
    "        \"Mystery\", \"Romance\", \"Sci-Fi\", \"Thriller\", \"War\", \"Western\"\n",
    "    ]\n",
    "    genre_dict = {genre: i + 1 for i, genre in enumerate(genre_list)}\n",
    "\n",
    "    movies[\"genre_vector\"] = movies[\"genres\"].apply(lambda x: [genre_dict[g] for g in x.split(\"|\") if g in genre_dict])\n",
    "    movie_dict = movies.set_index(\"movieId\")[\"genre_vector\"].to_dict()\n",
    "\n",
    "    return movie_dict, genre_dict\n",
    "\n",
    "def negative_sampling(movie_list, vocab_size, num_neg=2, max_len=30):\n",
    "    neg_samples = []\n",
    "    for _ in range(max_len):\n",
    "        neg = []\n",
    "        while len(neg) < num_neg:\n",
    "            sampled = np.random.randint(1, vocab_size)\n",
    "            if sampled not in movie_list:\n",
    "                neg.append(sampled)\n",
    "        neg_samples.append(neg)\n",
    "    return neg_samples  # Shape: (max_len, num_neg)\n",
    "\n",
    "class MovieDataset_UP(Dataset):\n",
    "    def __init__(self, user_movie_dict, users_dict, movie_dict, vocab_size, max_len=30, num_neg=2, max_genres=5):\n",
    "        self.users = list(user_movie_dict.keys())\n",
    "        self.sequences = [user_movie_dict[user] for user in self.users]\n",
    "        self.user_profiles = [users_dict[user] for user in self.users]\n",
    "        self.movie_dict = movie_dict\n",
    "        self.vocab_size = vocab_size\n",
    "        self.max_len = max_len\n",
    "        self.num_neg = num_neg\n",
    "        self.max_genres = max_genres\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.users)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence = self.sequences[idx]\n",
    "        user_profile = self.user_profiles[idx]\n",
    "\n",
    "        gender, age, occupation = (\n",
    "            user_profile[\"gender\"], user_profile[\"age\"], user_profile[\"occupation\"]\n",
    "        )\n",
    "\n",
    "        input_ids = sequence[:self.max_len] + [0] * (self.max_len - len(sequence))\n",
    "        target_ids = input_ids[1:] + [0]\n",
    "        attention_mask = [1 if id != 0 else 0 for id in input_ids]\n",
    "\n",
    "        # ✅ 外部调用negative_sampling\n",
    "        neg_samples = negative_sampling(sequence, self.vocab_size, self.num_neg, self.max_len)\n",
    "        neg_samples = torch.tensor(neg_samples, dtype=torch.long)\n",
    "\n",
    "        genre_vectors = [self.movie_dict.get(movie, [0]) for movie in input_ids]\n",
    "        padded_genre_vectors = [g[:self.max_genres] + [0] * (self.max_genres - len(g)) for g in genre_vectors]\n",
    "        genre_tensor = torch.tensor(padded_genre_vectors, dtype=torch.long)\n",
    "\n",
    "        return (\n",
    "            torch.tensor(input_ids, dtype=torch.long),\n",
    "            torch.tensor(target_ids, dtype=torch.long),\n",
    "            neg_samples,\n",
    "            torch.tensor(attention_mask, dtype=torch.long),\n",
    "            torch.tensor(gender, dtype=torch.long),\n",
    "            torch.tensor(age, dtype=torch.long),\n",
    "            torch.tensor(occupation, dtype=torch.long),\n",
    "            genre_tensor\n",
    "        )\n",
    "\n",
    "class BERT4Rec_UP(nn.Module):\n",
    "    def __init__(self, vocab_size, num_genres, hidden_size=256, num_layers=4, num_heads=4, max_len=30, dropout_rate=0.2):\n",
    "        super(BERT4Rec_UP, self).__init__()\n",
    "        config = BertConfig(\n",
    "            vocab_size=vocab_size, hidden_size=hidden_size, num_attention_heads=num_heads,\n",
    "            num_hidden_layers=num_layers, max_position_embeddings=max_len\n",
    "        )\n",
    "        self.bert = BertModel(config)\n",
    "        split_size = hidden_size // 3  # Adjusted due to removing activity embedding\n",
    "        self.gender_embedding = nn.Embedding(2, split_size)\n",
    "        self.age_embedding = nn.Embedding(7, split_size)\n",
    "        self.occupation_embedding = nn.Embedding(21, split_size)\n",
    "        self.genre_embedding = nn.Embedding(num_genres, hidden_size)\n",
    "        self.genre_fc = nn.Linear(hidden_size, hidden_size)\n",
    "        self.user_fc = nn.Linear(split_size * 3, hidden_size)\n",
    "        self.user_dropout = nn.Dropout(dropout_rate)\n",
    "        self.final_dropout = nn.Dropout(dropout_rate)\n",
    "        self.output_layer = nn.Linear(hidden_size, vocab_size)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, gender, age, occupation, genre_ids):\n",
    "        batch_size, seq_len = input_ids.shape\n",
    "        transformer_output = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        ).last_hidden_state\n",
    "        genre_emb = self.genre_embedding(genre_ids).mean(dim=2)\n",
    "        genre_emb = self.genre_fc(genre_emb)\n",
    "        user_emb = self.user_fc(torch.cat([\n",
    "            self.gender_embedding(gender).unsqueeze(1).expand(-1, seq_len, -1),\n",
    "            self.age_embedding(age).unsqueeze(1).expand(-1, seq_len, -1),\n",
    "            self.occupation_embedding(occupation).unsqueeze(1).expand(-1, seq_len, -1)\n",
    "        ], dim=-1))\n",
    "        user_emb = self.user_dropout(user_emb)\n",
    "        output = transformer_output + user_emb + genre_emb\n",
    "        return self.output_layer(self.final_dropout(output))\n",
    "\n",
    "# ✅ Updated Training Function\n",
    "def train_model_up(model, dataloader, epochs=10, lr=0.0001):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    criterion_ce = nn.CrossEntropyLoss(ignore_index=0)  # Used for positive samples\n",
    "    criterion_bce = nn.BCEWithLogitsLoss()  # Used for negative sampling\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n",
    "    early_stopping = EarlyStopping(patience=3)\n",
    "\n",
    "    model.train()\n",
    "    loss_history = [] \n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for inputs, targets, neg_samples, attention_mask, gender, age, occupation, genre_ids in dataloader:\n",
    "            inputs, targets, neg_samples, attention_mask, gender, age, occupation, genre_ids = (\n",
    "                inputs.to(device), targets.to(device), neg_samples.to(device),\n",
    "                attention_mask.to(device), gender.to(device), age.to(device),\n",
    "                occupation.to(device),\n",
    "                genre_ids.to(device)\n",
    "            )\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs, attention_mask, gender, age, occupation, genre_ids)  \n",
    "            # ✅ outputs.shape = [batch_size, seq_len, vocab_size]\n",
    "\n",
    "            # ✅ Compute Positive Sample Loss (CrossEntropy)\n",
    "            pos_loss = criterion_ce(outputs.view(-1, outputs.shape[-1]), targets.view(-1))  # Targets must be long integers\n",
    "\n",
    "            # ✅ Compute Negative Sample Loss (Binary Classification)\n",
    "            neg_logits = outputs.gather(2, neg_samples).squeeze(-1)  # Extract negative logits\n",
    "            neg_labels = torch.zeros_like(neg_logits)  # Label negative samples as \"0\"\n",
    "            neg_loss = criterion_bce(neg_logits, neg_labels.float())  # BCE expects float targets\n",
    "\n",
    "            # ✅ Compute Final Loss\n",
    "            loss = pos_loss + neg_loss.mean()\n",
    "\n",
    "            # ✅ Backpropagation\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        # ✅ Learning Rate Scheduling\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        loss_history.append(avg_loss)  # 记录 loss\n",
    "        scheduler.step(avg_loss)\n",
    "        print(f\"Epoch {epoch + 1}, Train Loss: {avg_loss:.4f}\")\n",
    "\n",
    "        # ✅ Early Stopping\n",
    "        if early_stopping.step(total_loss):\n",
    "            break\n",
    "    return loss_history  \n",
    "\n",
    "def evaluate_model_up(model, dataloader, k=10):\n",
    "    \"\"\"\n",
    "    Evaluate the BERT4Rec model on test data.\n",
    "\n",
    "    Args:\n",
    "        model: The trained BERT4Rec model.\n",
    "        dataloader: Test DataLoader.\n",
    "        k: Top-K predictions to consider for Recall and NDCG.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss, total_recall, total_ndcg = 0, 0, 0\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            # Unpack batch\n",
    "            inputs, targets, neg_samples, attention_mask, gender, age, occupation, genres = batch\n",
    "\n",
    "            # Move to device\n",
    "            inputs, targets, attention_mask = inputs.to(device), targets.to(device), attention_mask.to(device)\n",
    "            gender, age, occupation, genres = (\n",
    "                gender.to(device), age.to(device), occupation.to(device),\n",
    "                genres.to(device)\n",
    "            )\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs, attention_mask, gender, age, occupation, genres)\n",
    "            loss = criterion(outputs.view(-1, outputs.shape[-1]), targets.view(-1))\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Compute Recall@K & NDCG@K\n",
    "            _, top_k_predictions = torch.topk(outputs, k, dim=-1)  # Get top K movie predictions\n",
    "            recall = recall_at_k(top_k_predictions, targets, k)\n",
    "            ndcg = ndcg_at_k(top_k_predictions, targets, k)\n",
    "\n",
    "            total_recall += recall\n",
    "            total_ndcg += ndcg\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    avg_recall = total_recall / len(dataloader)\n",
    "    avg_ndcg = total_ndcg / len(dataloader)\n",
    "\n",
    "    print(f\"Test Loss: {avg_loss:.4f}, Recall@{k}: {avg_recall:.4f}, NDCG@{k}: {avg_ndcg:.4f}\")\n",
    "\n",
    "# ✅ Load datasets\n",
    "user_movie_dict, ratings_df = load_ratings()\n",
    "users_dict = load_users()\n",
    "movie_dict, genre_dict = load_movies()\n",
    "print(f\"Loaded {len(user_movie_dict)} users' movie interaction sequences.\")\n",
    "train_dict, test_dict = split_train_test_strict(user_movie_dict)\n",
    "print(f\"Train users: {len(train_dict)}, Test users: {len(test_dict)}\")\n",
    "vocab_size = max(max(seq) for seq in user_movie_dict.values()) + 1\n",
    "\n",
    "# ✅ Update DataLoaders with `genre_ids`\n",
    "train_dataset = MovieDataset_UP(train_dict, users_dict, movie_dict, vocab_size)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_dataset = MovieDataset_UP(test_dict, users_dict, movie_dict, vocab_size)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "vocab_size = max(max(seq) for seq in user_movie_dict.values()) + 1\n",
    "print(f\"Model initialized with vocab size {vocab_size}\")\n",
    "\n",
    "model_up = BERT4Rec_UP(vocab_size, len(genre_dict) + 1, dropout_rate=0.3).to(device)\n",
    "# ✅ Train the Model\n",
    "loss_history_up = train_model_up(model_up, train_dataloader, epochs=epochs, lr=0.0001)\n",
    "evaluate_model_up(model_up, test_dataloader, k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "69defd35-58e6-4919-8f78-1615bb3d9fd4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 6040 users' movie interaction sequences.\n",
      "Train users: 4832, Test users: 1208\n",
      "Model initialized with vocab size 3953 and 5 user clusters.\n",
      "Epoch 1, Train Loss: 8.0120\n",
      "Epoch 2, Train Loss: 7.1957\n",
      "Epoch 3, Train Loss: 6.7454\n",
      "Epoch 4, Train Loss: 6.2213\n",
      "Epoch 5, Train Loss: 5.0302\n",
      "Epoch 6, Train Loss: 3.3039\n",
      "Epoch 7, Train Loss: 2.1272\n",
      "Epoch 8, Train Loss: 1.3991\n",
      "Epoch 9, Train Loss: 0.9467\n",
      "Epoch 10, Train Loss: 0.6573\n",
      "Epoch 11, Train Loss: 0.4768\n",
      "Epoch 12, Train Loss: 0.3612\n",
      "Epoch 13, Train Loss: 0.2810\n",
      "Epoch 14, Train Loss: 0.2251\n",
      "Epoch 15, Train Loss: 0.1838\n",
      "Epoch 16, Train Loss: 0.1537\n",
      "Epoch 17, Train Loss: 0.1300\n",
      "Epoch 18, Train Loss: 0.1102\n",
      "Epoch 19, Train Loss: 0.0964\n",
      "Epoch 20, Train Loss: 0.0850\n",
      "Epoch 21, Train Loss: 0.0751\n",
      "Epoch 22, Train Loss: 0.0671\n",
      "Epoch 23, Train Loss: 0.0603\n",
      "Epoch 24, Train Loss: 0.0549\n",
      "Epoch 25, Train Loss: 0.0506\n",
      "Epoch 26, Train Loss: 0.0465\n",
      "Epoch 27, Train Loss: 0.0432\n",
      "Epoch 28, Train Loss: 0.0401\n",
      "Epoch 29, Train Loss: 0.0378\n",
      "Epoch 30, Train Loss: 0.0357\n",
      "Epoch 31, Train Loss: 0.0338\n",
      "Epoch 32, Train Loss: 0.0320\n",
      "Epoch 33, Train Loss: 0.0308\n",
      "Epoch 34, Train Loss: 0.0297\n",
      "Epoch 35, Train Loss: 0.0282\n",
      "Epoch 36, Train Loss: 0.0273\n",
      "Epoch 37, Train Loss: 0.0266\n",
      "Epoch 38, Train Loss: 0.0256\n",
      "Epoch 39, Train Loss: 0.0250\n",
      "Epoch 40, Train Loss: 0.0243\n",
      "Epoch 41, Train Loss: 0.0237\n",
      "Epoch 42, Train Loss: 0.0230\n",
      "Epoch 43, Train Loss: 0.0228\n",
      "Epoch 44, Train Loss: 0.0222\n",
      "Epoch 45, Train Loss: 0.0219\n",
      "Epoch 46, Train Loss: 0.0217\n",
      "Epoch 47, Train Loss: 0.0211\n",
      "Epoch 48, Train Loss: 0.0208\n",
      "Epoch 49, Train Loss: 0.0204\n",
      "Epoch 50, Train Loss: 0.0202\n",
      "Epoch 51, Train Loss: 0.0202\n",
      "Epoch 52, Train Loss: 0.0199\n",
      "Epoch 53, Train Loss: 0.0193\n",
      "Epoch 54, Train Loss: 0.0192\n",
      "Epoch 55, Train Loss: 0.0192\n",
      "Epoch 56, Train Loss: 0.0188\n",
      "Epoch 57, Train Loss: 0.0185\n",
      "Epoch 58, Train Loss: 0.0183\n",
      "Epoch 59, Train Loss: 0.0180\n",
      "Epoch 60, Train Loss: 0.0180\n",
      "Epoch 61, Train Loss: 0.0176\n",
      "Epoch 62, Train Loss: 0.0176\n",
      "Epoch 63, Train Loss: 0.0173\n",
      "Epoch 64, Train Loss: 0.0171\n",
      "Epoch 65, Train Loss: 0.0170\n",
      "Epoch 66, Train Loss: 0.0168\n",
      "Epoch 67, Train Loss: 0.0166\n",
      "Epoch 68, Train Loss: 0.0163\n",
      "Epoch 69, Train Loss: 0.0162\n",
      "Epoch 70, Train Loss: 0.0163\n",
      "Epoch 71, Train Loss: 0.0161\n",
      "Epoch 72, Train Loss: 0.0157\n",
      "Epoch 73, Train Loss: 0.0156\n",
      "Epoch 74, Train Loss: 0.0155\n",
      "Epoch 75, Train Loss: 0.0154\n",
      "Epoch 76, Train Loss: 0.0156\n",
      "Epoch 77, Train Loss: 0.0152\n",
      "Epoch 78, Train Loss: 0.0153\n",
      "Epoch 79, Train Loss: 0.0152\n",
      "Epoch 80, Train Loss: 0.0151\n",
      "Epoch 81, Train Loss: 0.0145\n",
      "Epoch 82, Train Loss: 0.0146\n",
      "Epoch 83, Train Loss: 0.0146\n",
      "Epoch 84, Train Loss: 0.0147\n",
      "Early stopping triggered.\n",
      "Test Loss: 0.0307, Recall@10: 0.9428, NDCG@10: 0.9428\n"
     ]
    }
   ],
   "source": [
    "#### BERT4Rec_UPC\n",
    "\n",
    "# ✅ Load MovieLens-1M Data (Users, Ratings, Movies)\n",
    "def load_ratings(filepath=\"ml-1m/ratings.dat\"):\n",
    "    df = pd.read_csv(filepath, sep=\"::\", engine=\"python\",\n",
    "                     names=[\"userId\", \"movieId\", \"rating\", \"timestamp\"],\n",
    "                     encoding=\"utf-8\")\n",
    "    df = df.sort_values(by=[\"userId\", \"timestamp\"])\n",
    "    user_movie_dict = df.groupby(\"userId\")[\"movieId\"].apply(list).to_dict()\n",
    "    return user_movie_dict, df\n",
    "\n",
    "def load_users(filepath=\"ml-1m/users.dat\"):\n",
    "    user_df = pd.read_csv(filepath, sep=\"::\", engine=\"python\",\n",
    "                           names=[\"userId\", \"gender\", \"age\", \"occupation\", \"zip-code\"],\n",
    "                           encoding=\"utf-8\")\n",
    "\n",
    "    gender_map = {\"M\": 0, \"F\": 1}\n",
    "    user_df[\"gender\"] = user_df[\"gender\"].map(gender_map)\n",
    "\n",
    "    age_groups = {1: 0, 18: 1, 25: 2, 35: 3, 45: 4, 50: 5, 56: 6}\n",
    "    user_df[\"age\"] = user_df[\"age\"].map(age_groups)\n",
    "\n",
    "    users_dict = user_df.set_index(\"userId\")[[\"gender\", \"age\", \"occupation\"]].to_dict(\"index\")\n",
    "    return users_dict\n",
    "\n",
    "def load_movies(filepath=\"ml-1m/movies.dat\"):\n",
    "    movies = pd.read_csv(filepath, sep=\"::\", engine=\"python\",\n",
    "                         names=[\"movieId\", \"title\", \"genres\"], encoding=\"latin-1\")\n",
    "\n",
    "    genre_list = [\n",
    "        \"Action\", \"Adventure\", \"Animation\", \"Children's\", \"Comedy\", \"Crime\",\n",
    "        \"Documentary\", \"Drama\", \"Fantasy\", \"Film-Noir\", \"Horror\", \"Musical\",\n",
    "        \"Mystery\", \"Romance\", \"Sci-Fi\", \"Thriller\", \"War\", \"Western\"\n",
    "    ]\n",
    "    genre_dict = {genre: i + 1 for i, genre in enumerate(genre_list)}\n",
    "\n",
    "    movies[\"genre_vector\"] = movies[\"genres\"].apply(lambda x: [genre_dict[g] for g in x.split(\"|\") if g in genre_dict])\n",
    "    movie_dict = movies.set_index(\"movieId\")[\"genre_vector\"].to_dict()\n",
    "\n",
    "    return movie_dict, genre_dict\n",
    "\n",
    "# ✅ Negative Sampling\n",
    "def negative_sampling(movie_list, vocab_size, num_neg=2, max_len=30):\n",
    "    neg_samples = []\n",
    "    for _ in range(max_len):\n",
    "        neg = []\n",
    "        while len(neg) < num_neg:\n",
    "            sampled = np.random.randint(1, vocab_size)\n",
    "            if sampled not in movie_list:\n",
    "                neg.append(sampled)\n",
    "        neg_samples.append(neg)\n",
    "    return neg_samples  # Shape: (max_len, num_neg)\n",
    "    \n",
    "class MovieDataset_UPC(Dataset):\n",
    "    def __init__(self, user_movie_dict, users_dict, movie_dict, vocab_size, max_len=30, num_neg=2, max_genres=5):\n",
    "        self.users = list(user_movie_dict.keys())\n",
    "        self.sequences = [user_movie_dict[user] for user in self.users]\n",
    "        self.user_profiles = [users_dict[user] for user in self.users]\n",
    "        self.movie_dict = movie_dict\n",
    "        self.vocab_size = vocab_size\n",
    "        self.max_len = max_len\n",
    "        self.num_neg = num_neg\n",
    "        self.max_genres = max_genres\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.users)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence = self.sequences[idx]\n",
    "        user_profile = self.user_profiles[idx]\n",
    "\n",
    "        gender, age, occupation, cluster = (\n",
    "            user_profile[\"gender\"], user_profile[\"age\"], user_profile[\"occupation\"],\n",
    "            user_profile[\"cluster\"]\n",
    "        )\n",
    "\n",
    "        input_ids = sequence[:self.max_len] + [0] * (self.max_len - len(sequence))\n",
    "        target_ids = input_ids[1:] + [0]\n",
    "        attention_mask = [1 if id != 0 else 0 for id in input_ids]\n",
    "\n",
    "        # ✅ 外部调用negative_sampling\n",
    "        neg_samples = negative_sampling(sequence, self.vocab_size, self.num_neg, self.max_len)\n",
    "        neg_samples = torch.tensor(neg_samples, dtype=torch.long)\n",
    "\n",
    "        genre_vectors = [self.movie_dict.get(movie, [0]) for movie in input_ids]\n",
    "        padded_genre_vectors = [g[:self.max_genres] + [0] * (self.max_genres - len(g)) for g in genre_vectors]\n",
    "        genre_tensor = torch.tensor(padded_genre_vectors, dtype=torch.long)\n",
    "\n",
    "        return (\n",
    "            torch.tensor(input_ids, dtype=torch.long),\n",
    "            torch.tensor(target_ids, dtype=torch.long),\n",
    "            neg_samples,\n",
    "            torch.tensor(attention_mask, dtype=torch.long),\n",
    "            torch.tensor(gender, dtype=torch.long),\n",
    "            torch.tensor(age, dtype=torch.long),\n",
    "            torch.tensor(occupation, dtype=torch.long),\n",
    "            torch.tensor(cluster, dtype=torch.long),\n",
    "            genre_tensor\n",
    "        )\n",
    "\n",
    "class BERT4Rec_UPC(nn.Module):\n",
    "    def __init__(self, vocab_size, num_clusters, num_genres, hidden_size=256, num_layers=4, num_heads=4, max_len=30, dropout_rate=0.2):\n",
    "        super(BERT4Rec_UPC, self).__init__()\n",
    "        config = BertConfig(\n",
    "            vocab_size=vocab_size, hidden_size=hidden_size, num_attention_heads=num_heads,\n",
    "            num_hidden_layers=num_layers, max_position_embeddings=max_len\n",
    "        )\n",
    "        self.bert = BertModel(config)\n",
    "        split_size = hidden_size // 4  # Adjusted due to removing activity embedding\n",
    "        self.gender_embedding = nn.Embedding(2, split_size)\n",
    "        self.age_embedding = nn.Embedding(7, split_size)\n",
    "        self.occupation_embedding = nn.Embedding(21, split_size)\n",
    "        self.cluster_embedding = nn.Embedding(num_clusters, split_size)\n",
    "        self.genre_embedding = nn.Embedding(num_genres, hidden_size)\n",
    "        self.genre_fc = nn.Linear(hidden_size, hidden_size)\n",
    "        self.user_fc = nn.Linear(split_size * 4, hidden_size)\n",
    "        self.user_dropout = nn.Dropout(dropout_rate)\n",
    "        self.final_dropout = nn.Dropout(dropout_rate)\n",
    "        self.output_layer = nn.Linear(hidden_size, vocab_size)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, gender, age, occupation, cluster, genre_ids):\n",
    "        batch_size, seq_len = input_ids.shape\n",
    "        causal_mask = torch.triu(torch.ones((seq_len, seq_len), dtype=torch.bool, device=input_ids.device), diagonal=1)\n",
    "        transformer_output = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            encoder_attention_mask=~causal_mask\n",
    "        ).last_hidden_state\n",
    "        genre_emb = self.genre_embedding(genre_ids).mean(dim=2)\n",
    "        genre_emb = self.genre_fc(genre_emb)\n",
    "        user_emb = self.user_fc(torch.cat([\n",
    "            self.gender_embedding(gender).unsqueeze(1).expand(-1, seq_len, -1),\n",
    "            self.age_embedding(age).unsqueeze(1).expand(-1, seq_len, -1),\n",
    "            self.occupation_embedding(occupation).unsqueeze(1).expand(-1, seq_len, -1),\n",
    "            self.cluster_embedding(cluster).unsqueeze(1).expand(-1, seq_len, -1)\n",
    "        ], dim=-1))\n",
    "        user_emb = self.user_dropout(user_emb)\n",
    "        output = transformer_output + user_emb + genre_emb\n",
    "        return self.output_layer(self.final_dropout(output))\n",
    "\n",
    "# ✅ Updated Training Function\n",
    "def train_model_upc(model, dataloader, epochs=10, lr=0.0001):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    criterion_ce = nn.CrossEntropyLoss(ignore_index=0)  # Used for positive samples\n",
    "    criterion_bce = nn.BCEWithLogitsLoss()  # Used for negative sampling\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n",
    "    early_stopping = EarlyStopping(patience=3)\n",
    "\n",
    "    model.train()\n",
    "    loss_history = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for inputs, targets, neg_samples, attention_mask, gender, age, occupation, cluster, genre_ids in dataloader:\n",
    "            inputs, targets, neg_samples, attention_mask, gender, age, occupation, cluster, genre_ids = (\n",
    "                inputs.to(device), targets.to(device), neg_samples.to(device),\n",
    "                attention_mask.to(device), gender.to(device), age.to(device),\n",
    "                occupation.to(device), cluster.to(device),\n",
    "                genre_ids.to(device)\n",
    "            )\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs, attention_mask, gender, age, occupation, cluster, genre_ids)  \n",
    "            # ✅ outputs.shape = [batch_size, seq_len, vocab_size]\n",
    "\n",
    "            # ✅ Compute Positive Sample Loss (CrossEntropy)\n",
    "            pos_loss = criterion_ce(outputs.view(-1, outputs.shape[-1]), targets.view(-1))  # Targets must be long integers\n",
    "\n",
    "            # ✅ Compute Negative Sample Loss (Binary Classification)\n",
    "            neg_logits = outputs.gather(2, neg_samples).squeeze(-1)  # Extract negative logits\n",
    "            neg_labels = torch.zeros_like(neg_logits)  # Label negative samples as \"0\"\n",
    "            neg_loss = criterion_bce(neg_logits, neg_labels.float())  # BCE expects float targets\n",
    "\n",
    "            # ✅ Compute Final Loss\n",
    "            loss = pos_loss + neg_loss.mean()\n",
    "\n",
    "            # ✅ Backpropagation\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        # ✅ Learning Rate Scheduling\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        loss_history.append(avg_loss) \n",
    "        scheduler.step(avg_loss)\n",
    "        print(f\"Epoch {epoch + 1}, Train Loss: {avg_loss:.4f}\")\n",
    "\n",
    "        # ✅ Early Stopping\n",
    "        if early_stopping.step(total_loss):\n",
    "            break\n",
    "    return loss_history  # 返回 loss 记录\n",
    "\n",
    "def evaluate_model_upc(model, dataloader, k=10):\n",
    "    \"\"\"\n",
    "    Evaluate the BERT4Rec model on test data.\n",
    "\n",
    "    Args:\n",
    "        model: The trained BERT4Rec model.\n",
    "        dataloader: Test DataLoader.\n",
    "        k: Top-K predictions to consider for Recall and NDCG.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss, total_recall, total_ndcg = 0, 0, 0\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            # Unpack batch\n",
    "            inputs, targets, neg_samples, attention_mask, gender, age, occupation, cluster, genres = batch\n",
    "\n",
    "            # Move to device\n",
    "            inputs, targets, attention_mask = inputs.to(device), targets.to(device), attention_mask.to(device)\n",
    "            gender, age, occupation, cluster, genres = (\n",
    "                gender.to(device), age.to(device), occupation.to(device), cluster.to(device),\n",
    "                genres.to(device)\n",
    "            )\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs, attention_mask, gender, age, occupation, cluster, genres)\n",
    "            loss = criterion(outputs.view(-1, outputs.shape[-1]), targets.view(-1))\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Compute Recall@K & NDCG@K\n",
    "            _, top_k_predictions = torch.topk(outputs, k, dim=-1)  # Get top K movie predictions\n",
    "            recall = recall_at_k(top_k_predictions, targets, k)\n",
    "            ndcg = ndcg_at_k(top_k_predictions, targets, k)\n",
    "\n",
    "            total_recall += recall\n",
    "            total_ndcg += ndcg\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    avg_recall = total_recall / len(dataloader)\n",
    "    avg_ndcg = total_ndcg / len(dataloader)\n",
    "\n",
    "    print(f\"Test Loss: {avg_loss:.4f}, Recall@{k}: {avg_recall:.4f}, NDCG@{k}: {avg_ndcg:.4f}\")\n",
    "\n",
    "# ✅ Load datasets\n",
    "user_movie_dict, ratings_df = load_ratings()\n",
    "users_dict = load_users()\n",
    "movie_dict, genre_dict = load_movies()\n",
    "\n",
    "print(f\"Loaded {len(user_movie_dict)} users' movie interaction sequences.\")\n",
    "train_dict, test_dict = split_train_test_strict(user_movie_dict)\n",
    "print(f\"Train users: {len(train_dict)}, Test users: {len(test_dict)}\")\n",
    "# ✅ Define vocab_size\n",
    "vocab_size = max(max(seq) for seq in user_movie_dict.values()) + 1\n",
    "\n",
    "# ✅ User Clustering using K-Means\n",
    "from sklearn.cluster import KMeans\n",
    "num_clusters = 5\n",
    "user_features = np.array([[users_dict[u][\"gender\"], users_dict[u][\"age\"], users_dict[u][\"occupation\"]] for u in users_dict.keys()])\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42, n_init=10)\n",
    "kmeans_clusters = kmeans.fit_predict(user_features)\n",
    "\n",
    "for idx, user_id in enumerate(users_dict.keys()):\n",
    "    users_dict[user_id][\"cluster\"] = kmeans_clusters[idx]\n",
    "    \n",
    "# ✅ Update DataLoaders with `genre_ids`\n",
    "train_dataset = MovieDataset_UPC(train_dict, users_dict, movie_dict, vocab_size)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_dataset = MovieDataset_UPC(test_dict, users_dict, movie_dict, vocab_size)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "vocab_size = max(max(seq) for seq in user_movie_dict.values()) + 1\n",
    "model_upc = BERT4Rec_UPC(vocab_size, num_clusters, len(genre_dict) + 1, dropout_rate=0.3).to(device)\n",
    "print(f\"Model initialized with vocab size {vocab_size} and {num_clusters} user clusters.\")\n",
    "\n",
    "# ✅ Train the Model\n",
    "loss_history_upc = train_model_upc(model_upc, train_dataloader, epochs=epochs, lr=0.0001)\n",
    "evaluate_model_upc(model_upc, test_dataloader, k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "41c66ae8-587e-49b6-8333-6046c84a9379",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 6040 users' movie interaction sequences.\n",
      "Train users: 4832, Test users: 1208\n",
      "Model initialized with vocab size 3953 and 5 user clusters.\n",
      "Epoch 1, Train Loss: 7.9747\n",
      "Epoch 2, Train Loss: 7.1968\n",
      "Epoch 3, Train Loss: 6.7581\n",
      "Epoch 4, Train Loss: 6.2441\n",
      "Epoch 5, Train Loss: 5.0679\n",
      "Epoch 6, Train Loss: 3.3471\n",
      "Epoch 7, Train Loss: 2.1751\n",
      "Epoch 8, Train Loss: 1.4497\n",
      "Epoch 9, Train Loss: 0.9908\n",
      "Epoch 10, Train Loss: 0.6934\n",
      "Epoch 11, Train Loss: 0.5019\n",
      "Epoch 12, Train Loss: 0.3814\n",
      "Epoch 13, Train Loss: 0.2947\n",
      "Epoch 14, Train Loss: 0.2360\n",
      "Epoch 15, Train Loss: 0.1919\n",
      "Epoch 16, Train Loss: 0.1595\n",
      "Epoch 17, Train Loss: 0.1348\n",
      "Epoch 18, Train Loss: 0.1153\n",
      "Epoch 19, Train Loss: 0.0999\n",
      "Epoch 20, Train Loss: 0.0879\n",
      "Epoch 21, Train Loss: 0.0782\n",
      "Epoch 22, Train Loss: 0.0697\n",
      "Epoch 23, Train Loss: 0.0625\n",
      "Epoch 24, Train Loss: 0.0574\n",
      "Epoch 25, Train Loss: 0.0520\n",
      "Epoch 26, Train Loss: 0.0483\n",
      "Epoch 27, Train Loss: 0.0445\n",
      "Epoch 28, Train Loss: 0.0417\n",
      "Epoch 29, Train Loss: 0.0386\n",
      "Epoch 30, Train Loss: 0.0365\n",
      "Epoch 31, Train Loss: 0.0347\n",
      "Epoch 32, Train Loss: 0.0332\n",
      "Epoch 33, Train Loss: 0.0314\n",
      "Epoch 34, Train Loss: 0.0303\n",
      "Epoch 35, Train Loss: 0.0291\n",
      "Epoch 36, Train Loss: 0.0278\n",
      "Epoch 37, Train Loss: 0.0270\n",
      "Epoch 38, Train Loss: 0.0262\n",
      "Epoch 39, Train Loss: 0.0255\n",
      "Epoch 40, Train Loss: 0.0248\n",
      "Epoch 41, Train Loss: 0.0242\n",
      "Epoch 42, Train Loss: 0.0237\n",
      "Epoch 43, Train Loss: 0.0233\n",
      "Epoch 44, Train Loss: 0.0227\n",
      "Epoch 45, Train Loss: 0.0221\n",
      "Epoch 46, Train Loss: 0.0217\n",
      "Epoch 47, Train Loss: 0.0217\n",
      "Epoch 48, Train Loss: 0.0212\n",
      "Epoch 49, Train Loss: 0.0207\n",
      "Epoch 50, Train Loss: 0.0204\n",
      "Epoch 51, Train Loss: 0.0202\n",
      "Epoch 52, Train Loss: 0.0201\n",
      "Epoch 53, Train Loss: 0.0196\n",
      "Epoch 54, Train Loss: 0.0193\n",
      "Epoch 55, Train Loss: 0.0192\n",
      "Epoch 56, Train Loss: 0.0191\n",
      "Epoch 57, Train Loss: 0.0187\n",
      "Epoch 58, Train Loss: 0.0187\n",
      "Epoch 59, Train Loss: 0.0181\n",
      "Epoch 60, Train Loss: 0.0180\n",
      "Epoch 61, Train Loss: 0.0179\n",
      "Epoch 62, Train Loss: 0.0178\n",
      "Epoch 63, Train Loss: 0.0176\n",
      "Epoch 64, Train Loss: 0.0175\n",
      "Epoch 65, Train Loss: 0.0171\n",
      "Epoch 66, Train Loss: 0.0170\n",
      "Epoch 67, Train Loss: 0.0170\n",
      "Epoch 68, Train Loss: 0.0167\n",
      "Epoch 69, Train Loss: 0.0166\n",
      "Epoch 70, Train Loss: 0.0162\n",
      "Epoch 71, Train Loss: 0.0163\n",
      "Epoch 72, Train Loss: 0.0161\n",
      "Epoch 73, Train Loss: 0.0162\n",
      "Epoch 74, Train Loss: 0.0159\n",
      "Epoch 75, Train Loss: 0.0161\n",
      "Epoch 76, Train Loss: 0.0154\n",
      "Epoch 77, Train Loss: 0.0156\n",
      "Epoch 78, Train Loss: 0.0154\n",
      "Epoch 79, Train Loss: 0.0153\n",
      "Epoch 80, Train Loss: 0.0151\n",
      "Epoch 81, Train Loss: 0.0150\n",
      "Epoch 82, Train Loss: 0.0151\n",
      "Epoch 83, Train Loss: 0.0152\n",
      "Epoch 84, Train Loss: 0.0148\n",
      "Epoch 85, Train Loss: 0.0146\n",
      "Epoch 86, Train Loss: 0.0145\n",
      "Epoch 87, Train Loss: 0.0145\n",
      "Epoch 88, Train Loss: 0.0146\n",
      "Epoch 89, Train Loss: 0.0142\n",
      "Epoch 90, Train Loss: 0.0143\n",
      "Epoch 91, Train Loss: 0.0142\n",
      "Epoch 92, Train Loss: 0.0142\n",
      "Epoch 93, Train Loss: 0.0140\n",
      "Epoch 94, Train Loss: 0.0138\n",
      "Epoch 95, Train Loss: 0.0140\n",
      "Epoch 96, Train Loss: 0.0140\n",
      "Epoch 97, Train Loss: 0.0140\n",
      "Early stopping triggered.\n",
      "Test Loss: 0.0281, Recall@10: 0.9422, NDCG@10: 0.9422\n"
     ]
    }
   ],
   "source": [
    "### BERT4REC_UPCA\n",
    "\n",
    "# ✅ Load MovieLens-1M Data (Users, Ratings, Movies)\n",
    "def load_ratings(filepath=\"ml-1m/ratings.dat\"):\n",
    "    df = pd.read_csv(filepath, sep=\"::\", engine=\"python\",\n",
    "                     names=[\"userId\", \"movieId\", \"rating\", \"timestamp\"],\n",
    "                     encoding=\"utf-8\")\n",
    "    df = df.sort_values(by=[\"userId\", \"timestamp\"])\n",
    "    user_movie_dict = df.groupby(\"userId\")[\"movieId\"].apply(list).to_dict()\n",
    "    return user_movie_dict, df\n",
    "\n",
    "def load_users(filepath=\"ml-1m/users.dat\"):\n",
    "    user_df = pd.read_csv(filepath, sep=\"::\", engine=\"python\",\n",
    "                           names=[\"userId\", \"gender\", \"age\", \"occupation\", \"zip-code\"],\n",
    "                           encoding=\"utf-8\")\n",
    "\n",
    "    gender_map = {\"M\": 0, \"F\": 1}\n",
    "    user_df[\"gender\"] = user_df[\"gender\"].map(gender_map)\n",
    "\n",
    "    age_groups = {1: 0, 18: 1, 25: 2, 35: 3, 45: 4, 50: 5, 56: 6}\n",
    "    user_df[\"age\"] = user_df[\"age\"].map(age_groups)\n",
    "\n",
    "    users_dict = user_df.set_index(\"userId\")[[\"gender\", \"age\", \"occupation\"]].to_dict(\"index\")\n",
    "    return users_dict\n",
    "\n",
    "def load_movies(filepath=\"ml-1m/movies.dat\"):\n",
    "    movies = pd.read_csv(filepath, sep=\"::\", engine=\"python\",\n",
    "                         names=[\"movieId\", \"title\", \"genres\"], encoding=\"latin-1\")\n",
    "\n",
    "    genre_list = [\n",
    "        \"Action\", \"Adventure\", \"Animation\", \"Children's\", \"Comedy\", \"Crime\",\n",
    "        \"Documentary\", \"Drama\", \"Fantasy\", \"Film-Noir\", \"Horror\", \"Musical\",\n",
    "        \"Mystery\", \"Romance\", \"Sci-Fi\", \"Thriller\", \"War\", \"Western\"\n",
    "    ]\n",
    "    genre_dict = {genre: i + 1 for i, genre in enumerate(genre_list)}\n",
    "\n",
    "    movies[\"genre_vector\"] = movies[\"genres\"].apply(lambda x: [genre_dict[g] for g in x.split(\"|\") if g in genre_dict])\n",
    "    movie_dict = movies.set_index(\"movieId\")[\"genre_vector\"].to_dict()\n",
    "\n",
    "    return movie_dict, genre_dict\n",
    "\n",
    "# ✅ Negative Sampling\n",
    "def negative_sampling(movie_list, vocab_size, num_neg=2, max_len=30):\n",
    "    neg_samples = []\n",
    "    for _ in range(max_len):\n",
    "        neg = []\n",
    "        while len(neg) < num_neg:\n",
    "            sampled = np.random.randint(1, vocab_size)\n",
    "            if sampled not in movie_list:\n",
    "                neg.append(sampled)\n",
    "        neg_samples.append(neg)\n",
    "    return neg_samples  # Shape: (max_len, num_neg)\n",
    "\n",
    "class MovieDataset_UPCA(Dataset):\n",
    "    def __init__(self, user_movie_dict, users_dict, movie_dict, vocab_size, max_len=30, num_neg=2, max_genres=5):\n",
    "        self.users = list(user_movie_dict.keys())\n",
    "        self.sequences = [user_movie_dict[user] for user in self.users]\n",
    "        self.user_profiles = [users_dict[user] for user in self.users]\n",
    "        self.movie_dict = movie_dict\n",
    "        self.vocab_size = vocab_size\n",
    "        self.max_len = max_len\n",
    "        self.num_neg = num_neg\n",
    "        self.max_genres = max_genres\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.users)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence = self.sequences[idx]\n",
    "        user_profile = self.user_profiles[idx]\n",
    "\n",
    "        gender, age, occupation, cluster, activity_level = (\n",
    "            user_profile[\"gender\"], user_profile[\"age\"], user_profile[\"occupation\"],\n",
    "            user_profile[\"cluster\"], user_profile[\"activity_level\"]\n",
    "        )\n",
    "\n",
    "        input_ids = sequence[:self.max_len] + [0] * (self.max_len - len(sequence))\n",
    "        target_ids = input_ids[1:] + [0]\n",
    "        attention_mask = [1 if id != 0 else 0 for id in input_ids]\n",
    "\n",
    "        # ✅ 外部调用negative_sampling\n",
    "        neg_samples = negative_sampling(sequence, self.vocab_size, self.num_neg, self.max_len)\n",
    "        neg_samples = torch.tensor(neg_samples, dtype=torch.long)\n",
    "\n",
    "        genre_vectors = [self.movie_dict.get(movie, [0]) for movie in input_ids]\n",
    "        padded_genre_vectors = [g[:self.max_genres] + [0] * (self.max_genres - len(g)) for g in genre_vectors]\n",
    "        genre_tensor = torch.tensor(padded_genre_vectors, dtype=torch.long)\n",
    "\n",
    "        return (\n",
    "            torch.tensor(input_ids, dtype=torch.long),\n",
    "            torch.tensor(target_ids, dtype=torch.long),\n",
    "            neg_samples,\n",
    "            torch.tensor(attention_mask, dtype=torch.long),\n",
    "            torch.tensor(gender, dtype=torch.long),\n",
    "            torch.tensor(age, dtype=torch.long),\n",
    "            torch.tensor(occupation, dtype=torch.long),\n",
    "            torch.tensor(cluster, dtype=torch.long),\n",
    "            torch.tensor(activity_level, dtype=torch.long),\n",
    "            genre_tensor\n",
    "        )\n",
    "\n",
    "# ✅ Corrected BERT4Rec Model with User & Genre Embeddings\n",
    "class BERT4Rec_UPCA(nn.Module):\n",
    "    def __init__(self, vocab_size, num_clusters, num_genres, hidden_size=256, num_layers=4, num_heads=4, max_len=30, dropout_rate=0.2):\n",
    "        super(BERT4Rec_UPCA, self).__init__()\n",
    "\n",
    "        # ✅ Transformer Configuration\n",
    "        config = BertConfig(\n",
    "            vocab_size=vocab_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_attention_heads=num_heads,\n",
    "            num_hidden_layers=num_layers,\n",
    "            max_position_embeddings=max_len,\n",
    "        )\n",
    "\n",
    "        # ✅ Transformer Encoder\n",
    "        self.bert = BertModel(config)\n",
    "        \n",
    "        # ✅ User Feature Embeddings\n",
    "        split_size = hidden_size // 5  \n",
    "        self.gender_embedding = nn.Embedding(2, split_size)\n",
    "        self.age_embedding = nn.Embedding(7, split_size)\n",
    "        self.occupation_embedding = nn.Embedding(21, split_size)\n",
    "        self.cluster_embedding = nn.Embedding(num_clusters, split_size)\n",
    "        self.activity_embedding = nn.Embedding(2, split_size)\n",
    "\n",
    "        # ✅ Movie Genre Embeddings\n",
    "        self.genre_embedding = nn.Embedding(num_genres, hidden_size)\n",
    "        self.genre_fc = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "        # ✅ Fully Connected Layer for User Features\n",
    "        self.user_fc = nn.Linear(split_size * 5, hidden_size)\n",
    "\n",
    "        # ✅ Dropout Regularization\n",
    "        self.user_dropout = nn.Dropout(dropout_rate)\n",
    "        self.final_dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "        # ✅ Output Layer (Predict Next Movie)\n",
    "        self.output_layer = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, gender, age, occupation, cluster, activity_level, genre_ids):\n",
    "        batch_size, seq_len = input_ids.shape\n",
    "\n",
    "        # ✅ Causal Mask (Prevents Information Leakage)\n",
    "        causal_mask = torch.triu(torch.ones(seq_len, seq_len, dtype=torch.bool, device=input_ids.device), diagonal=1)\n",
    "\n",
    "        # ✅ Transformer Output\n",
    "        transformer_output = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            encoder_attention_mask=~causal_mask\n",
    "        ).last_hidden_state\n",
    "\n",
    "        # ✅ Movie Genre Embedding\n",
    "        genre_emb = self.genre_embedding(genre_ids).mean(dim=2)  \n",
    "        genre_emb = self.genre_fc(genre_emb)\n",
    "\n",
    "        # ✅ User Profile Embeddings (Expand to Sequence Length)\n",
    "        user_emb = torch.cat([\n",
    "            self.gender_embedding(gender).unsqueeze(1).expand(-1, seq_len, -1),\n",
    "            self.age_embedding(age).unsqueeze(1).expand(-1, seq_len, -1),\n",
    "            self.occupation_embedding(occupation).unsqueeze(1).expand(-1, seq_len, -1),\n",
    "            self.cluster_embedding(cluster).unsqueeze(1).expand(-1, seq_len, -1),\n",
    "            self.activity_embedding(activity_level).unsqueeze(1).expand(-1, seq_len, -1)\n",
    "        ], dim=-1)\n",
    "\n",
    "        # ✅ Fully Connected User Features\n",
    "        user_emb = self.user_fc(user_emb)\n",
    "        user_emb = self.user_dropout(user_emb)  \n",
    "\n",
    "        # ✅ Inject User & Genre Information into Transformer Output\n",
    "        output = transformer_output + user_emb + genre_emb  \n",
    "        output = self.final_dropout(output)\n",
    "\n",
    "        # ✅ Predict Next Movie\n",
    "        return self.output_layer(output)\n",
    "\n",
    "# ✅ Updated Training Function\n",
    "def train_model_upca(model, dataloader, epochs=10, lr=0.0001):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    criterion_ce = nn.CrossEntropyLoss(ignore_index=0)  # Used for positive samples\n",
    "    criterion_bce = nn.BCEWithLogitsLoss()  # Used for negative sampling\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n",
    "    early_stopping = EarlyStopping(patience=3)\n",
    "\n",
    "    model.train()\n",
    "    loss_history = [] \n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for inputs, targets, neg_samples, attention_mask, gender, age, occupation, cluster, activity_level, genre_ids in dataloader:\n",
    "            inputs, targets, neg_samples, attention_mask, gender, age, occupation, cluster, activity_level, genre_ids = (\n",
    "                inputs.to(device), targets.to(device), neg_samples.to(device),\n",
    "                attention_mask.to(device), gender.to(device), age.to(device),\n",
    "                occupation.to(device), cluster.to(device), activity_level.to(device),\n",
    "                genre_ids.to(device)\n",
    "            )\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs, attention_mask, gender, age, occupation, cluster, activity_level, genre_ids)  \n",
    "            # ✅ outputs.shape = [batch_size, seq_len, vocab_size]\n",
    "\n",
    "            # ✅ Compute Positive Sample Loss (CrossEntropy)\n",
    "            pos_loss = criterion_ce(outputs.view(-1, outputs.shape[-1]), targets.view(-1))  # Targets must be long integers\n",
    "\n",
    "            # ✅ Compute Negative Sample Loss (Binary Classification)\n",
    "            neg_logits = outputs.gather(2, neg_samples).squeeze(-1)  # Extract negative logits\n",
    "            neg_labels = torch.zeros_like(neg_logits)  # Label negative samples as \"0\"\n",
    "            neg_loss = criterion_bce(neg_logits, neg_labels.float())  # BCE expects float targets\n",
    "\n",
    "            # ✅ Compute Final Loss\n",
    "            loss = pos_loss + neg_loss.mean()\n",
    "\n",
    "            # ✅ Backpropagation\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        # ✅ Learning Rate Scheduling\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        loss_history.append(avg_loss)  # 记录 loss\n",
    "        scheduler.step(avg_loss)\n",
    "        print(f\"Epoch {epoch + 1}, Train Loss: {avg_loss:.4f}\")\n",
    "\n",
    "        # ✅ Early Stopping\n",
    "        if early_stopping.step(total_loss):\n",
    "            break\n",
    "    return loss_history \n",
    "\n",
    "# Define Evaluation Function\n",
    "def evaluate_model_upca(model, dataloader, k=10):\n",
    "    \"\"\"\n",
    "    Evaluate the BERT4Rec model on test data.\n",
    "\n",
    "    Args:\n",
    "        model: The trained BERT4Rec model.\n",
    "        dataloader: Test DataLoader.\n",
    "        k: Top-K predictions to consider for Recall and NDCG.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss, total_recall, total_ndcg = 0, 0, 0\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            # Unpack batch\n",
    "            inputs, targets, neg_samples, attention_mask, gender, age, occupation, cluster, activity_level, genres = batch\n",
    "\n",
    "            # Move to device\n",
    "            inputs, targets, attention_mask = inputs.to(device), targets.to(device), attention_mask.to(device)\n",
    "            gender, age, occupation, cluster, activity_level, genres = (\n",
    "                gender.to(device), age.to(device), occupation.to(device), cluster.to(device),\n",
    "                activity_level.to(device), genres.to(device)\n",
    "            )\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs, attention_mask, gender, age, occupation, cluster, activity_level, genres)\n",
    "            loss = criterion(outputs.view(-1, outputs.shape[-1]), targets.view(-1))\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Compute Recall@K & NDCG@K\n",
    "            _, top_k_predictions = torch.topk(outputs, k, dim=-1)  # Get top K movie predictions\n",
    "            recall = recall_at_k(top_k_predictions, targets, k)\n",
    "            ndcg = ndcg_at_k(top_k_predictions, targets, k)\n",
    "\n",
    "            total_recall += recall\n",
    "            total_ndcg += ndcg\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    avg_recall = total_recall / len(dataloader)\n",
    "    avg_ndcg = total_ndcg / len(dataloader)\n",
    "\n",
    "    print(f\"Test Loss: {avg_loss:.4f}, Recall@{k}: {avg_recall:.4f}, NDCG@{k}: {avg_ndcg:.4f}\")\n",
    "\n",
    "\n",
    "# ✅ Load datasets\n",
    "user_movie_dict, ratings_df = load_ratings()\n",
    "users_dict = load_users()\n",
    "movie_dict, genre_dict = load_movies()\n",
    "print(f\"Loaded {len(user_movie_dict)} users' movie interaction sequences.\")\n",
    "train_dict, test_dict = split_train_test_strict(user_movie_dict)\n",
    "print(f\"Train users: {len(train_dict)}, Test users: {len(test_dict)}\")\n",
    "\n",
    "# ✅ User Clustering using K-Means\n",
    "num_clusters = 5\n",
    "user_features = np.array([[users_dict[u][\"gender\"], users_dict[u][\"age\"], users_dict[u][\"occupation\"]] for u in users_dict.keys()])\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42, n_init=10)\n",
    "kmeans_clusters = kmeans.fit_predict(user_features)\n",
    "\n",
    "for idx, user_id in enumerate(users_dict.keys()):\n",
    "    users_dict[user_id][\"cluster\"] = kmeans_clusters[idx]\n",
    "\n",
    "# ✅ User Activity Level (Median-Based)\n",
    "def classify_users(user_movie_dict, percentile=50):\n",
    "    user_activity = np.array([len(movies) for movies in user_movie_dict.values()])\n",
    "    threshold = np.percentile(user_activity, percentile)\n",
    "\n",
    "    high_activity_users = {user for user, movies in user_movie_dict.items() if len(movies) >= threshold}\n",
    "    low_activity_users = set(user_movie_dict.keys()) - high_activity_users\n",
    "\n",
    "    return high_activity_users, low_activity_users\n",
    "\n",
    "high_activity_users, low_activity_users = classify_users(user_movie_dict)\n",
    "for user_id in users_dict.keys():\n",
    "    users_dict[user_id][\"activity_level\"] = 1 if user_id in high_activity_users else 0\n",
    "\n",
    "# ✅ Update DataLoaders with `genre_ids`\n",
    "train_dataset = MovieDataset_UPCA(train_dict, users_dict, movie_dict, vocab_size)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_dataset = MovieDataset_UPCA(test_dict, users_dict, movie_dict, vocab_size)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# ✅ Initialize Model\n",
    "vocab_size = max(max(seq) for seq in user_movie_dict.values()) + 1\n",
    "model_upca = BERT4Rec_UPCA(vocab_size, num_clusters, len(genre_dict) + 1, dropout_rate=0.3).to(device)\n",
    "print(f\"Model initialized with vocab size {vocab_size} and {num_clusters} user clusters.\")\n",
    "# ✅ Train the Model\n",
    "loss_history_upca = train_model_upca(model_upca, train_dataloader, epochs=epochs, lr=0.0001)\n",
    "evaluate_model_upca(model_upca, test_dataloader, k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e2814c0d-c676-4a79-8c30-1586e92a5ac5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_loss_curves(losses_dict):\n",
    "    \"\"\"\n",
    "    Plots loss curves for multiple models.\n",
    "    \n",
    "    Args:\n",
    "        losses_dict (dict): Dictionary containing model names as keys and lists of loss values as values.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    for model_name, losses in losses_dict.items():\n",
    "        plt.plot(range(1, len(losses) + 1), losses, marker='o', label=model_name)\n",
    "    \n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training Loss Curves\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Example usage with sample data (replace with actual loss values from training)\n",
    "losses_dict = {\n",
    "    \"BERT4Rec\": loss_history_baseline,\n",
    "    \"BERT4Rec_UP\": loss_history_up,\n",
    "    \"BERT4Rec_UPC\": loss_history_upc,\n",
    "    \"BERT4Rec_UPCA\": loss_history_upca\n",
    "}\n",
    "\n",
    "plot_loss_curves(losses_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "45151821-8b06-4e04-aceb-4d1e09a056e9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Genre Embedding Variance: 0.035228412598371506\n",
      "Cluster 4: 3367 unique movies\n",
      "Cluster 1: 3498 unique movies\n",
      "Cluster 2: 3380 unique movies\n",
      "Cluster 0: 3528 unique movies\n",
      "Cluster 3: 3450 unique movies\n",
      "Distribution of unique genres per user:\n",
      "[(5, 1), (6, 1), (7, 10), (8, 15), (9, 52), (10, 126), (11, 214), (12, 327), (13, 448), (14, 588), (15, 668), (16, 855), (17, 1306), (18, 1429)]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Genre Embedding Variance: {torch.var(model_baseline.genre_embedding.weight).item()}\")\n",
    "\n",
    "import collections\n",
    "\n",
    "# Check how many unique movies each user cluster watches\n",
    "cluster_movie_counts = collections.defaultdict(set)\n",
    "for user, movies in user_movie_dict.items():\n",
    "    cluster = users_dict[user][\"cluster\"]\n",
    "    cluster_movie_counts[cluster].update(movies)\n",
    "\n",
    "for cluster, movies in cluster_movie_counts.items():\n",
    "    print(f\"Cluster {cluster}: {len(movies)} unique movies\")\n",
    "    \n",
    "import collections\n",
    "\n",
    "genre_counts = collections.defaultdict(int)\n",
    "for user, movies in user_movie_dict.items():\n",
    "    user_genres = set()\n",
    "    for movie in movies:\n",
    "        user_genres.update(movie_dict.get(movie, []))\n",
    "    genre_counts[len(user_genres)] += 1\n",
    "\n",
    "print(\"Distribution of unique genres per user:\")\n",
    "print(sorted(genre_counts.items()))  # (Number of genres, Count of users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd21b32-8c03-4c86-9961-f98a35a9a719",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
